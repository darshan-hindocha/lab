{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Reinforcement Learning on Atari.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darshan-hindocha/lab/blob/main/deep_reinforcement_learning_on_atari.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVw18KU2PbTU"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this implementation I use a Deep Q-Network to learn the best policy for the Atari game 'freeway', through OpenAI's Gym library.\n",
        "\n",
        "I supplement the Deep Q-Network with Experience Replay and Batch Sampling. I use an epsilon-greedy type explore-exploit policy.\n",
        "\n",
        "I use tensorflow (keras in particular) for the underlying function approximator that replaces the Q-table in a non-Deep reinforcement learning project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hFtZ6mdJsdk"
      },
      "source": [
        "## Run on Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-AxnvAVyzQQ"
      },
      "source": [
        "# @title Installing ATARI\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdb2JwZy4jGj"
      },
      "source": [
        "#@title Imports\n",
        "import time\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from skimage.transform import resize\n",
        "from skimage.color import rgb2gray\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\n",
        "\n",
        "import skimage.transform\n",
        "import skimage.measure\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.regularizers import l2\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import keras\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9UWeToN4r7D"
      },
      "source": [
        "def wrap_env(env,ep_num='sample'):\n",
        "  env = Monitor(env, f'./video{ep_num}', force=True)\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZWY0HjG-ACL"
      },
      "source": [
        "### Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtB70Sl7rQfs"
      },
      "source": [
        "def grayscale_resize(image):\n",
        "\n",
        "  img = resize(rgb2gray(image[0:195,:,:]), (60, 60),mode='constant')\n",
        "  img = np.reshape(img, [60,60,1])\n",
        "\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD_hxS3ip7s4"
      },
      "source": [
        "def build_cnn():\n",
        "\n",
        "  img_input = layers.Input(shape=(60, 240, 1))\n",
        "\n",
        "  x = layers.Conv2D(16, 3, activation='relu')(img_input)\n",
        "  x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "  x = layers.Conv2D(32, 3, activation='relu')(x)\n",
        "  x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "  x = layers.Conv2D(64, 3, activation='relu')(x)\n",
        "  x = layers.MaxPooling2D(2)(x)\n",
        "\n",
        "  x = layers.Flatten()(x)\n",
        "\n",
        "  x = layers.Dense(512, activation='relu')(x)\n",
        "\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "\n",
        "  output = layers.Dense(1)(x)\n",
        "\n",
        "  model = Model(img_input, output)\n",
        "  model.compile(loss='mean_absolute_error',optimizer=tf.keras.optimizers.Adam(0.001))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5clUMUiG4nN"
      },
      "source": [
        "### Function Approximator CNN\n",
        "This is the function that maps values in the state-action space to values. It needs three main parts, the initialisation, the prediction, and finally it needs an update method.\n",
        "\n",
        "- Initialise: set up the function arbitrarily\n",
        "- Update parameters by taking in state, action, and actual values. Actual value will be compared to the value at the state and action to create a numeric loss which is used to update the weights. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFHlLCLBrhuV"
      },
      "source": [
        "class value_function():\n",
        "\n",
        "  def __init__(self):\n",
        "    ##Create the models\n",
        "    self.models = np.empty(3,dtype=object)\n",
        "    self.untrainable_models = np.empty(3,dtype=object)\n",
        "    for i in range(3):\n",
        "      self.models[i] = build_cnn()\n",
        "      \n",
        "    \n",
        "  def init_models(self,states):\n",
        "\n",
        "    ##Initialise the models with arbitrary weights\n",
        "\n",
        "    input_states = np.hstack(states).reshape(1,60,240,1)\n",
        "    for i in range(3):\n",
        "      self.models[i].fit(x=input_states,y=np.array([0]),epochs=1,verbose=0)\n",
        "      self.models[i].save(f\"model{i}\")\n",
        "\n",
        "      ## Create model replicas to use for predictions\n",
        "      self.untrainable_models[i] = keras.models.load_model(f\"model{i}\")\n",
        "    pass\n",
        "    \n",
        "  def update(self,memory_box):\n",
        "    '''\n",
        "    Memory Box - the list of experiences [obs0,obs1,obs2,obs3,action,td]\n",
        "    '''\n",
        "    ## Decide which experiences should update which model\n",
        "    all_actions = np.array([exp[4] for exp in memory_box])\n",
        "\n",
        "    ## Formatting to get experiences input ready\n",
        "    for i in range(3):\n",
        "      idx = np.where(all_actions==i)[0]\n",
        "      if len(idx)>0:\n",
        "        model_input = []\n",
        "        model_target = []\n",
        "        for inp in idx:\n",
        "          model_input.append(np.hstack([memory_box[inp][0],memory_box[inp][1],memory_box[inp][2],memory_box[inp][3]]).reshape(60,240,1))\n",
        "          model_target.append(memory_box[inp][5])\n",
        "\n",
        "        model_input = np.array(model_input)\n",
        "        model_target = np.array(model_target)\n",
        "\n",
        "        ## Update the keras model\n",
        "        self.models[i].fit(model_input,model_target)\n",
        "        self.models[i].save(f\"model{i}\")\n",
        "\n",
        "        ##load weights to untrainable model\n",
        "        self.untrainable_models[i] = keras.models.load_model(f\"model{i}\")\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "  def predict_best(self,states):\n",
        "    '''\n",
        "    Take in an experience and return the values of each\n",
        "    '''\n",
        "    vals = np.zeros(3)\n",
        "    for i in range(3):\n",
        "      vals[i] = self.untrainable_models[i].predict(np.hstack(states).reshape(1,60,240,1))\n",
        "    return vals\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "penmjA14EiQz"
      },
      "source": [
        "### Agent\n",
        "\n",
        "$ V = Reward + \\gamma*Max_{a}Q(S',a)$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soeDkR6wHSss"
      },
      "source": [
        "class Q_agent():\n",
        "  def __init__(self, environment,value_func, gamma = 0.9, epsilon = 0.05):\n",
        "    '''\n",
        "    initialise hyperparameters\n",
        "\n",
        "    input (environment, value_function_approximator, gamma, epsilon)\n",
        "    '''\n",
        "    ## Set the hyperparameters    \n",
        "    self.epsilon = epsilon\n",
        "    self.gamma = gamma\n",
        "    self.value_function = value_func\n",
        "\n",
        "    ##Initialise the function approximators\n",
        "    self.environment = gym.make(\"Freeway-v0\")\n",
        "    state = self.environment.reset()\n",
        "    state = grayscale_resize(state)\n",
        "    states = [state,state,state,state]\n",
        "    self.value_function.init_models(states)\n",
        "\n",
        "    ## Initialise the memory where the sampled experiences are stored\n",
        "    self.memory_box = []\n",
        "    pass\n",
        "  \n",
        "  def run_episode(self,ep = 2000):\n",
        "\n",
        "    ##initialise the environment\n",
        "    self.environment = gym.make(\"Freeway-v0\")\n",
        "    state = self.environment.reset()\n",
        "    state = grayscale_resize(state)\n",
        "\n",
        "    ## First 4 frames of an episode are not used for prediction or updates\n",
        "    initial = 0\n",
        "    cumulative_return = 0\n",
        "\n",
        "    ## state compilations are the experiences\n",
        "    state_compilations = np.array([state,state,state,state])\n",
        "    terminal = False\n",
        "\n",
        "    ## end is used to end the episode after 5 crossings\n",
        "    end = 0\n",
        "\n",
        "    while not terminal:\n",
        "      \n",
        "      ## Choosing the action\n",
        "        \n",
        "        ## we need 4 frames/observations to call predict so first 4 frames do not call predict\n",
        "      if initial < 4: \n",
        "        action = np.random.choice(3)\n",
        "        \n",
        "        ## epsilon greedy policy - Epsilon % of the time a random action is taken\n",
        "      elif np.random.uniform()<self.epsilon: \n",
        "        action = np.random.choice(3)\n",
        "        \n",
        "        ## choose actions using the predict function\n",
        "      else:            \n",
        "        state_action_values = self.value_function.predict_best(state_compilations)\n",
        "        action = np.argmax(state_action_values)\n",
        "\n",
        "      \n",
        "      ## Taking the action in the environment\n",
        "      obs, reward, terminal, _ = self.environment.step(action)\n",
        "\n",
        "      ## Altered the rewards as described in the report\n",
        "      if reward == 0:\n",
        "        reward = -1\n",
        "      else:\n",
        "        reward = 100\n",
        "        end += 1\n",
        "      cumulative_return+=reward\n",
        "      \n",
        "      \n",
        "      obs = grayscale_resize(obs)\n",
        "      ##adding the observation to our temporary experience memory\n",
        "      if initial < 4:\n",
        "        state_compilations[initial] = obs\n",
        "      else:\n",
        "        state_compilations[:3] = state_compilations[1:]\n",
        "        state_compilations[3] = obs\n",
        "\n",
        "      ## calculating the estimated value of our observation\n",
        "      if initial < 4:\n",
        "        initial+=1\n",
        "        V=reward\n",
        "      else:\n",
        "        ## Equation described in Markdown above\n",
        "        V = reward + self.gamma*np.max(self.value_function.predict_best(state_compilations))\n",
        "      \n",
        "      ## Taking a sample for replay experience 10% of the Time\n",
        "      if np.random.uniform()<0.1 and initial >= 4:\n",
        "        self.memory_box.append([state_compilations[0],state_compilations[1],state_compilations[2],state_compilations[3],action,V])\n",
        "\n",
        "        ## if the memory box has enough samples then do an update\n",
        "        if len(self.memory_box) == 32:\n",
        "          self.value_function.update(self.memory_box)\n",
        "          self.memory_box = []\n",
        "\n",
        "      ## For early termination\n",
        "      if end == 5:\n",
        "        terminal = True\n",
        "      if count > 1000:\n",
        "        terminal = True\n",
        "      count+=1\n",
        "\n",
        "    ## close the environment\n",
        "    self.environment.close()\n",
        "    return cumulative_return\n",
        "\n",
        "  ## Run a simulation\n",
        "  def run_simulation(self, n_episodes=2):\n",
        "\n",
        "    start_time = time.time()\n",
        "    self.episodic_rewards = np.zeros(n_episodes)\n",
        "    self.times = np.zeros(n_episodes)\n",
        "    \n",
        "    for ep in range(n_episodes):\n",
        "      start_time = time.time()\n",
        "      self.episodic_rewards[ep] = self.run_episode(ep)\n",
        "      self.times[ep] = time.time() - start_time\n",
        "\n",
        "    return self.episodic_rewards,self.times\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsM_itFXJhBh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0c2d92be-4443-44c1-8b53-38f3972c2b58"
      },
      "source": [
        "## Set q_ag as the agent\n",
        "q_ag = Q_agent(gym.make(\"Freeway-v0\"),value_function())\n",
        "\n",
        "base_start_time = time.time()\n",
        "\n",
        "## Run simulation\n",
        "episodic_rewards,time_list = q_ag.run_simulation(n_episodes= 75)\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - base_start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "state action value: [ 0.86908972 -1.70917666  1.40470862]\n",
            "state action value: [ 0.8838442  -1.72082281  1.41324961]\n",
            "state action value: [ 0.88593596 -1.72375035  1.4165839 ]\n",
            "state action value: [ 0.88692456 -1.72196805  1.41555119]\n",
            "state action value: [ 0.87754732 -1.71324265  1.40702248]\n",
            "state action value: [ 0.88763243 -1.7206738   1.41852558]\n",
            "state action value: [ 0.88198644 -1.72149849  1.41450953]\n",
            "state action value: [ 0.89082551 -1.72452903  1.41651821]\n",
            "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_train_function.<locals>.train_function at 0x7f123d70f830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.8153\n",
            "INFO:tensorflow:Assets written to: model0/assets\n",
            "WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_train_function.<locals>.train_function at 0x7f123b5d3050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 271ms/step - loss: 1.7727\n",
            "INFO:tensorflow:Assets written to: model1/assets\n",
            "WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_train_function.<locals>.train_function at 0x7f123b89cc20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 1s 620ms/step - loss: 1.0989\n",
            "INFO:tensorflow:Assets written to: model2/assets\n",
            "state action value: [ 0.44418833 -1.14612627  0.52087927]\n",
            "state action value: [ 0.4426547  -1.15056312  0.5232228 ]\n",
            "state action value: [ 0.44144359 -1.15191913  0.52299297]\n",
            "state action value: [ 0.4491834  -1.15501249  0.52332515]\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.9588\n",
            "INFO:tensorflow:Assets written to: model0/assets\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.0811\n",
            "INFO:tensorflow:Assets written to: model1/assets\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 1.0021\n",
            "INFO:tensorflow:Assets written to: model2/assets\n",
            "state action value: [-0.12620999 -0.34165597 -0.36046207]\n",
            "state action value: [-0.13060907 -0.34123003 -0.36399248]\n",
            "state action value: [-0.12908581 -0.34210762 -0.36480993]\n",
            "state action value: [-0.1295345  -0.34437701 -0.36341682]\n",
            "state action value: [-0.12848811 -0.34074783 -0.36131799]\n",
            "state action value: [-0.12850441 -0.34279585 -0.36435753]\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 0.9854\n",
            "INFO:tensorflow:Assets written to: model0/assets\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 1.1187\n",
            "INFO:tensorflow:Assets written to: model1/assets\n",
            "state action value: [-0.78559327 -0.28737184 -0.36171964]\n",
            "state action value: [-0.79831332 -0.28636357 -0.36734468]\n",
            "-1002.0\n",
            "--- 324.5476818084717 seconds ---\n",
            "state action value: [-0.76236629 -0.28485548 -0.35777098]\n",
            "state action value: [-0.78395963 -0.28796101 -0.36225015]\n",
            "state action value: [-0.78326523 -0.28787783 -0.36411029]\n",
            "state action value: [-0.78080386 -0.28583482 -0.36633387]\n",
            "state action value: [-0.78910404 -0.28932554 -0.36476231]\n",
            "state action value: [-0.78449023 -0.28955346 -0.3673203 ]\n",
            "1/1 [==============================] - 0s 338ms/step - loss: 0.9959\n",
            "INFO:tensorflow:Assets written to: model1/assets\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.8909\n",
            "INFO:tensorflow:Assets written to: model2/assets\n",
            "state action value: [-0.77873743 -0.40563965 -1.17623687]\n",
            "state action value: [-0.7875393  -0.40830144 -1.18839407]\n",
            "state action value: [-0.78013849 -0.40133265 -1.17925489]\n",
            "state action value: [-0.78570914 -0.40908447 -1.18778515]\n",
            "state action value: [-0.78938872 -0.40813258 -1.18485749]\n",
            "201.0\n",
            "--- 177.03357362747192 seconds ---\n",
            "state action value: [-0.76243031 -0.40119466 -1.16422868]\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.3389\n",
            "INFO:tensorflow:Assets written to: model0/assets\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 0.9834\n",
            "INFO:tensorflow:Assets written to: model1/assets\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4148\n",
            "INFO:tensorflow:Assets written to: model2/assets\n",
            "state action value: [-1.54066455 -0.59787506 -2.20760274]\n",
            "state action value: [-1.54850817 -0.59880269 -2.21910501]\n",
            "state action value: [-1.54207599 -0.59958464 -2.22291064]\n",
            "state action value: [-1.54713202 -0.60092562 -2.22144532]\n",
            "state action value: [-1.542974   -0.60178602 -2.22395301]\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.5770\n",
            "INFO:tensorflow:Assets written to: model0/assets\n",
            "1/1 [==============================] - 0s 323ms/step - loss: 5.7091\n",
            "INFO:tensorflow:Assets written to: model1/assets\n",
            "state action value: [-1.58542478 -0.92464954 -2.21158552]\n",
            "state action value: [-1.60920608 -0.93015522 -2.22812748]\n",
            "state action value: [-1.59250152 -0.92386997 -2.21575332]\n",
            "state action value: [-1.57656538 -0.92769331 -2.22029877]\n",
            "state action value: [-1.59423411 -0.92777854 -2.22015905]\n",
            "205.0\n",
            "--- 183.353089094162 seconds ---\n",
            "state action value: [-1.54749072 -0.91388017 -2.18843222]\n",
            "state action value: [-1.59685922 -0.92642611 -2.21199417]\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 0.9369\n",
            "INFO:tensorflow:Assets written to: model1/assets\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6939\n",
            "INFO:tensorflow:Assets written to: model2/assets\n",
            "state action value: [-1.60373855 -1.4832536  -2.23979473]\n",
            "state action value: [-1.59595025 -1.48261595 -2.24692202]\n",
            "state action value: [-1.6029824  -1.48210013 -2.24444866]\n",
            "state action value: [-1.58941662 -1.47763681 -2.22638726]\n",
            "state action value: [-1.59056664 -1.48209703 -2.235255  ]\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.4751\n",
            "INFO:tensorflow:Assets written to: model0/assets\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.8412\n",
            "INFO:tensorflow:Assets written to: model1/assets\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.4384\n",
            "INFO:tensorflow:Assets written to: model2/assets\n",
            "state action value: [-1.98938739 -2.37969255 -2.06440568]\n",
            "state action value: [-1.97831988 -2.37283897 -2.05895352]\n",
            "state action value: [-1.979774   -2.37860966 -2.06443977]\n",
            "state action value: [-1.97450268 -2.3727777  -2.06036496]\n",
            "state action value: [-1.98568022 -2.37764454 -2.06308222]\n",
            "state action value: [-1.96759856 -2.36982989 -2.05461335]\n",
            "1/1 [==============================] - 0s 354ms/step - loss: 0.7341\n",
            "INFO:tensorflow:Assets written to: model0/assets\n",
            "state action value: [-2.72763371 -2.38158512 -2.06505084]\n",
            "state action value: [-2.71276426 -2.37676525 -2.06080031]\n",
            "state action value: [-2.71304107 -2.37319422 -2.05539775]\n",
            "state action value: [-2.71309495 -2.37478304 -2.05907583]\n",
            "state action value: [-2.72116804 -2.38005304 -2.06459141]\n",
            "1/1 [==============================] - 0s 337ms/step - loss: 0.8251\n",
            "INFO:tensorflow:Assets written to: model2/assets\n",
            "state action value: [-2.68962765 -2.37022376 -2.40482712]\n",
            "-700.0\n",
            "--- 340.49387073516846 seconds ---\n",
            "state action value: [-2.64406776 -2.35753942 -2.39011049]\n",
            "state action value: [-2.72185993 -2.38683867 -2.42926693]\n",
            "state action value: [-2.69340467 -2.37577415 -2.411654  ]\n",
            "state action value: [-2.70051908 -2.38270855 -2.42450666]\n",
            "state action value: [-2.69233656 -2.3680799  -2.40213823]\n",
            "state action value: [-2.71940303 -2.38427305 -2.42321825]\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.2152\n",
            "INFO:tensorflow:Assets written to: model0/assets\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.7756\n",
            "INFO:tensorflow:Assets written to: model1/assets\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.3596\n",
            "INFO:tensorflow:Assets written to: model2/assets\n",
            "state action value: [-3.8843596  -3.77026272 -3.1805613 ]\n",
            "state action value: [-3.90807486 -3.76437616 -3.17425466]\n",
            "state action value: [-3.86229038 -3.74388218 -3.15208912]\n",
            "state action value: [-3.87504148 -3.75538683 -3.16604376]\n",
            "state action value: [-3.87539148 -3.75608015 -3.16347075]\n",
            "state action value: [-3.90648699 -3.76591372 -3.17402935]\n",
            "state action value: [-3.87560177 -3.75128651 -3.16431856]\n",
            "state action value: [-3.90210271 -3.76317453 -3.17269874]\n",
            "1/1 [==============================] - 0s 345ms/step - loss: 0.6258\n",
            "INFO:tensorflow:Assets written to: model2/assets\n",
            "state action value: [-3.88424468 -3.77429485 -4.36706543]\n",
            "state action value: [-3.83388495 -3.74791503 -4.33020496]\n",
            "state action value: [-3.8509922  -3.76013207 -4.34767294]\n",
            "state action value: [-3.90436268 -3.77302742 -4.36586952]\n",
            "state action value: [-3.89473271 -3.77386355 -4.37003279]\n",
            "-398.0\n",
            "--- 326.6048822402954 seconds ---\n",
            "state action value: [-3.81475282 -3.73346257 -4.30953884]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d0f7f48f0e04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbase_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mepisodic_rewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_ag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodic_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodic_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-58ddbdac8dd9>\u001b[0m in \u001b[0;36mrun_simulation\u001b[0;34m(self, start_ep, n_episodes)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodic_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodic_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-58ddbdac8dd9>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(self, ep)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#choose actions using the predict function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mstate_action_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_best\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_compilations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misdf\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'state action value:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_action_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-48e6000c818e>\u001b[0m in \u001b[0;36mpredict_best\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m       \u001b[0mvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muntrainable_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m240\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1623\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1625\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1626\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    680\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    703\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 705\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m       \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   2970\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 2972\u001b[0;31m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[1;32m   2973\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}