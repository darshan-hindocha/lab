{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Boosting and Custom Kernel SVM Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darshan-hindocha/lab/blob/main/boosting_and_custom_kernel_SVM_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz3vk_ZGsgJ7"
      },
      "source": [
        "Image Dataset can be found [here](https://drive.google.com/file/d/146Rmq_3LJfyEsPNCvLhw8rCLP_Oxavje/view?usp=sharing)\n",
        "\n",
        "Movie Review Dataset can be found [here](https://drive.google.com/file/d/1XECfdzosU6p_koY39jnvMAO6GdHnAfCt/view?usp=sharing)"
      ],
      "id": "Oz3vk_ZGsgJ7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqCA8pNFPJut",
        "outputId": "c3b7e05f-8c58-4ca4-eaa9-39a5d11576dc"
      },
      "source": [
        "\n",
        "from zipfile import ZipFile\n",
        "file_name = 'image_dataset.zip'\n",
        "\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')"
      ],
      "id": "EqCA8pNFPJut",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "requested-certification"
      },
      "source": [
        "#### Image feature extraction code"
      ],
      "id": "requested-certification"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "korean-booth",
        "outputId": "421bcdea-4a85-4377-ea87-d303f9a0c746"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import KFold\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwordlist = stopwords.words('english')\n",
        "import string\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "    \n",
        "def obtain_dataset(folder_name):\n",
        "    labels_count = 0\n",
        "    labels = {}\n",
        "    # assuming 128x128 size images and HoGDescriptor length of 34020\n",
        "    hog_feature_len=34020\n",
        "    hog = cv2.HOGDescriptor()\n",
        "    #code for obtaining hog feature for one image file name\n",
        "    # im = cv2.imread(image_filename)\n",
        "    # h = hog.compute(image)\n",
        "    # use this to read all images in the three directories and obtain the set of features X and train labels Y\n",
        "    # you can assume there are three different classes in the image dataset\n",
        "    X=[]\n",
        "    y=[]\n",
        "    for image_filename in glob.iglob(folder_name+r'/*/*.png', recursive=True):\n",
        "        im = cv2.imread(image_filename)\n",
        "        h = hog.compute(im)\n",
        "        X.append(h)\n",
        "        \n",
        "        label = image_filename.split('/')[1]\n",
        "        if label not in labels:\n",
        "            labels[label] = labels_count\n",
        "            labels_count+=1\n",
        "        y.append(labels[label])\n",
        "\n",
        "    X = np.array(X).reshape(150,34020)\n",
        "    y = np.array(y).reshape(-1)\n",
        "\n",
        "    return (X,y) "
      ],
      "id": "korean-booth",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "professional-protection"
      },
      "source": [
        "#### Boosting classifier class"
      ],
      "id": "professional-protection"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "narrative-tolerance"
      },
      "source": [
        "class BoostingClassifier:\n",
        "  # You need to implement this classifier. \n",
        "  def __init__(self, feature_reduction = True,scaling=False, samplesize=22,num_classifiers=14,max_tree_depth=10,max_features='sqrt'):\n",
        "    #implement initialisation\n",
        "    self.samplesize = samplesize\n",
        "    self.num_weak_classifiers = num_classifiers\n",
        "    self.tree_depth = max_tree_depth\n",
        "    self.n_features = 'sqrt'\n",
        "    self.epsilon = 0.001\n",
        "    self.feature_reduction = feature_reduction\n",
        "    self.scale = scaling\n",
        "\n",
        "  def choose_samples(self,N):\n",
        "    sample_indices = np.random.choice(N,size = self.samplesize, replace = True, p = self.weights)\n",
        "    X_bag = self.Xtrain[sample_indices,:]\n",
        "    y_bag = self.ytrain[sample_indices]\n",
        "    return X_bag, y_bag\n",
        "\n",
        "  def fit_and_run_model(self,X_bag, y_bag,i):\n",
        "    self.classifiers[i] = DecisionTreeClassifier(max_depth = self.tree_depth,max_features=self.n_features,min_samples_split=5)\n",
        "    self.classifiers[i].fit(X_bag,y_bag)\n",
        "\n",
        "    y_pred = self.classifiers[i].predict(self.Xtrain)\n",
        "    misclassified = (y_pred!=self.ytrain)\n",
        "    total_error = np.sum(y_pred!=self.ytrain)/(y_pred.shape[0])\n",
        "\n",
        "    self.model_alphas[i] = np.log(float(\"{:.4f}\".format((1-total_error)/(total_error))))\n",
        "    return misclassified\n",
        "\n",
        "  \n",
        "  def reweight(self, misclassified, clf_performance):\n",
        "    self.weights[misclassified] = self.weights[misclassified]*np.exp(clf_performance)\n",
        "    \n",
        "    correctly_classified = np.invert(misclassified)\n",
        "    self.weights[correctly_classified] = self.weights[correctly_classified]*np.exp(-clf_performance)\n",
        "\n",
        "    #normalise\n",
        "    self.weights = self.weights/sum(self.weights)\n",
        "    pass\n",
        "  \n",
        "  def fit(self, X,y):\n",
        "    #implement training of the boosting classifier\n",
        "    #initialise - Training data, weak classifiers, performances, weights\n",
        "    \n",
        "    N = X.shape[0]\n",
        "    self.Xtrain = np.array(X)\n",
        "    self.ytrain = np.array(y).reshape(-1)\n",
        "\n",
        "    self.classifiers = np.empty((self.num_weak_classifiers),dtype=object)\n",
        "    self.model_alphas = np.empty((self.num_weak_classifiers))\n",
        "    \n",
        "    self.weights = (1/N)*np.ones(N) #equal weights\n",
        "\n",
        "    if self.scale:\n",
        "      #Standardise\n",
        "      self.scaler = preprocessing.StandardScaler().fit(self.Xtrain)\n",
        "      self.Xtrain = self.scaler.transform(self.Xtrain)\n",
        "    if self.feature_reduction:\n",
        "      #PCA\n",
        "      self.pca = PCA()\n",
        "      self.Xtrain = self.pca.fit_transform(self.Xtrain,self.ytrain)\n",
        "\n",
        "\n",
        "    for i in range(self.num_weak_classifiers):\n",
        "      #choose samples for a weak classifier\n",
        "      X_bag,y_bag = self.choose_samples(N)\n",
        "      \n",
        "      #run the model, get the performance alpha and the misclassified vector\n",
        "      misclassified = self.fit_and_run_model(X_bag,y_bag,i)\n",
        "      \n",
        "      #reweight the weights for the next round\n",
        "      self.reweight(misclassified,self.model_alphas[i])\n",
        "    pass\n",
        "\n",
        "\n",
        "  def cross_validation(self,X,y,feature_reduction,Scaled,TreeDepth,SampleSize,WeakClassifiers):\n",
        "\n",
        "    self.samplesize = SampleSize\n",
        "    self.num_weak_classifiers = WeakClassifiers\n",
        "    self.tree_depth = TreeDepth\n",
        "    self.feature_reduction = feature_reduction\n",
        "    self.scale = Scaled\n",
        "    \n",
        "    # prepare cross validation\n",
        "    kfold = KFold(10,True,3)\n",
        "    acc = []\n",
        "    # enumerate splits\n",
        "    for train, validate in kfold.split(X):\n",
        "      self.fit(X[train],y[train])\n",
        "      pred = self.predict(X[validate])\n",
        "      acc.append(accuracy_score(y[validate], pred))  \n",
        "    return np.mean(acc)\n",
        "    \n",
        "\n",
        "\n",
        "  def predict(self, X,Transform=True):\n",
        "    # implement prediction of the boosting classifier\n",
        "    if Transform:\n",
        "      if self.scale: X = self.scaler.transform(X)\n",
        "      if self.feature_reduction: X = self.pca.transform(X)\n",
        "    \n",
        "    y_pred = np.empty((X.shape[0],self.num_weak_classifiers))\n",
        "    \n",
        "    for i,clf in enumerate(self.classifiers):\n",
        "      y_pred[:,i] = clf.predict(X)\n",
        "\n",
        "\n",
        "    ret = np.empty((X.shape[0],3))\n",
        "    ret[:,0] = np.sum(np.multiply(self.model_alphas,y_pred==0),axis = 1)\n",
        "    ret[:,1] = np.sum(np.multiply(self.model_alphas,y_pred==1),axis = 1)\n",
        "    ret[:,2] = np.sum(np.multiply(self.model_alphas,y_pred==2),axis = 1)\n",
        "\n",
        "    ret = np.argmax(ret,axis=1)\n",
        "\n",
        "    return ret"
      ],
      "id": "narrative-tolerance",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30FNGUz2x_El"
      },
      "source": [
        "#### Hyper-Parameter Tuning"
      ],
      "id": "30FNGUz2x_El"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ5ne7aUST5o"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "from google.colab import files \n",
        "df = pd.DataFrame(columns=['PCA','Scaled','TreeDepth','SampleSize','No.WeakClassifiers','Accuracy'])\n",
        "counter = 0\n",
        "for sample_size in reversed(range(4,40,2)):\n",
        "  for num_of_classifiers in reversed(range(2,25,3)):\n",
        "    for tree_depth in reversed(range(2,20,4)):\n",
        "      for feature_reduction in [1,0]:\n",
        "        for scaled in [1,0]:\n",
        "          try:\n",
        "            bc = BoostingClassifier()\n",
        "            acc = bc.cross_validation(Xtrain,ytrain,feature_reduction,scaled,tree_depth,sample_size,num_of_classifiers)\n",
        "            new_row = {'PCA':feature_reduction,'Scaled':scaled,'TreeDepth':tree_depth,'SampleSize':sample_size,'No.WeakClassifiers':num_of_classifiers,'Accuracy':acc}\n",
        "          except Exception as err:\n",
        "            new_row = {'PCA':feature_reduction,'Scaled':scaled,'TreeDepth':tree_depth,'SampleSize':sample_size,'No.WeakClassifiers':num_of_classifiers,'Accuracy':err}\n",
        "          \n",
        "          df = df.append(new_row,ignore_index=True)\n",
        "          counter+=1\n",
        "          if counter%200 == 0:\n",
        "            df.to_csv(f\"backboostingtuning{counter}.csv\")\n",
        "            files.download(f\"backboostingtuning{counter}.csv\")\n",
        "```\n",
        "\n"
      ],
      "id": "mJ5ne7aUST5o"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "occasional-retailer"
      },
      "source": [
        "#### Obtain the train,test split"
      ],
      "id": "occasional-retailer"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "theoretical-float"
      },
      "source": [
        "train_folder_name='image_dataset'\n",
        "(X_train, Y_train) = obtain_dataset(train_folder_name)\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X_train, Y_train, train_size=0.8, random_state=123)"
      ],
      "id": "theoretical-float",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "physical-solid"
      },
      "source": [
        "#### Train the boosting classifer and evaluate it on the train-test split"
      ],
      "id": "physical-solid"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "competent-brighton",
        "outputId": "a48610a2-d8fa-4fc0-8eea-8faeb2f99e05"
      },
      "source": [
        "bc = BoostingClassifier()\n",
        "bc.fit(Xtrain, ytrain)\n",
        "y_pred = bc.predict(Xtest)\n",
        "print('accuracy', accuracy_score(ytest, y_pred))"
      ],
      "id": "competent-brighton",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.43333333333333335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fossil-expense"
      },
      "source": [
        "### Test function that will be called to evaluate."
      ],
      "id": "fossil-expense"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyric-myanmar"
      },
      "source": [
        "def test_func_boosting_image(image_dataset_train, image_dataset_test):\n",
        "    (X_train, Y_train) = obtain_dataset(image_dataset_train)\n",
        "    (X_test, Y_test) = obtain_dataset(image_dataset_test)\n",
        "    bc = BoostingClassifier()\n",
        "    bc.fit(X_train, Y_train)\n",
        "    y_pred = bc.predict(X_test)\n",
        "    print('accuracy', accuracy_score(Y_test, y_pred))\n",
        "    return y_pred"
      ],
      "id": "lyric-myanmar",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "concrete-upgrade"
      },
      "source": [
        "# Task 2\n",
        "\n",
        "Classify the above dataset using a Support Vector Machine (SVM).\n"
      ],
      "id": "concrete-upgrade"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyl5J4eltkf3"
      },
      "source": [
        "### Kernels"
      ],
      "id": "vyl5J4eltkf3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dng2hAIBZ3fZ"
      },
      "source": [
        "sig = 3\n",
        "def exp_kernel(X2,X1,sigma=sig):\n",
        "  func = lambda a,b: np.exp(-np.linalg.norm(a-b)/(2*(sigma**0.5)))\n",
        "  hXX = np.apply_along_axis(lambda x1 : np.apply_along_axis(lambda x2:  func(x1, x2), 1, X1), 1, X2)\n",
        "  return hXX\n",
        "\n",
        "def laplacian_kernel(X2,X1,sigma=sig):\n",
        "  func = lambda a,b: np.exp(-np.linalg.norm(a-b)/sigma)\n",
        "  hXX = np.apply_along_axis(lambda x1 : np.apply_along_axis(lambda x2:  func(x1, x2), 1, X1), 1, X2)\n",
        "  return hXX\n",
        "\n",
        "def rational_quadratic_kernel(X2,X1,sigma=sig):\n",
        "  func = lambda a,b: 1 - (np.linalg.norm(a-b)**2)/((np.linalg.norm(a-b)**2)+sigma)\n",
        "  hXX = np.apply_along_axis(lambda x1 : np.apply_along_axis(lambda x2:  func(x1, x2), 1, X1), 1, X2)\n",
        "  return hXX\n",
        "\n",
        "def log_kernel(X2,X1,sigma=sig):\n",
        "  func = lambda a,b: - np.log((np.linalg.norm(a-b)**sigma)+1)\n",
        "  hXX = np.apply_along_axis(lambda x1 : np.apply_along_axis(lambda x2:  func(x1, x2), 1, X1), 1, X2)\n",
        "  return hXX\n",
        "\n",
        "def spline_kernel(X2,X1):\n",
        "  func = lambda a,b,c: np.prod(1 + a*b + a*b*c - (((a+b)/2)*(c**2)) + ((c**3)/3))\n",
        "  hXX = np.apply_along_axis(lambda x1: np.apply_along_axis(lambda x2:  func(x1, x2,np.minimum(x1,x2)), 1, X1), 1, X2)\n",
        "  return hXX\n",
        "\n",
        "def cauchy_kernel(X2,X1,sigma=sig):\n",
        "  func = lambda a,b: 1/(1+(np.linalg.norm(a-b)**2)/(sigma**2))\n",
        "  hXX = np.apply_along_axis(lambda x1: np.apply_along_axis(lambda x2:  func(x1, x2), 1, X1), 1, X2)\n",
        "\n",
        "  return hXX\n",
        "\n"
      ],
      "id": "Dng2hAIBZ3fZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqEFQaFYx6jx"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "from google.colab import files \n",
        "df = pd.DataFrame(columns=['PCA','Scaled','Kernel','Sigma','Accuracy'])\n",
        "counter = 0\n",
        "for kern in reversed(['linear', 'rbf','sigmoid','poly',exp_kernel,laplacian_kernel,rational_quadratic_kernel,log_kernel,spline_kernel,cauchy_kernel]):\n",
        "  for feature_reduction in [1,0]:\n",
        "    for scaled in [1,0]:\n",
        "      for sig in np.random.uniform(0.1,10,size=5):\n",
        "        if kern not in [spline_kernel] and not isinstance(kern,str): kern.__defaults__ = (_,_,sig)\n",
        "        try:\n",
        "          sv = SVMClassifier()\n",
        "          acc = sv.cross_validation(Xtrain,ytrain,feature_reduction,scaled,'image',kern)\n",
        "          if not isinstance(kern,str):\n",
        "            new_row = {'PCA':feature_reduction,'Scaled':scaled,'Kernel':kern.__name__,'Sigma':sig,'Accuracy':acc}\n",
        "          else: new_row = {'PCA':feature_reduction,'Scaled':scaled,'Kernel':kern,'Sigma':sig,'Accuracy':acc}\n",
        "        except Exception as err:\n",
        "          if not isinstance(kern,str):\n",
        "            new_row = {'PCA':feature_reduction,'Scaled':scaled,'Kernel':kern.__name__,'Sigma':sig,'Accuracy':err}\n",
        "          else: new_row = {'PCA':feature_reduction,'Scaled':scaled,'Kernel':kern,'Sigma':sig,'Accuracy':err}\n",
        "        \n",
        "        df = df.append(new_row,ignore_index=True)\n",
        "        counter+=1\n",
        "        if counter%20 == 0:\n",
        "          df.to_csv(f\"svmtuning{counter}.csv\")\n",
        "          files.download(f\"svmtuning{counter}.csv\")\n",
        "        if kern in [spline_kernel] or isinstance(kern,str): break\n",
        "```\n",
        "\n"
      ],
      "id": "UqEFQaFYx6jx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "remarkable-physiology"
      },
      "source": [
        "from sklearn import svm\n",
        "\n",
        "class SVMClassifier:\n",
        "  def __init__(self,kern='linear',feature_reduction=True, Scaled=True):\n",
        "    #implement initialisation\n",
        "    self.kern = kern\n",
        "    self.feature_reduction = feature_reduction\n",
        "    self.scale = Scaled\n",
        "    pass\n",
        "  \n",
        "  def fit_image(self,X,y,kern=log_kernel,feature_reduction=True, Scaled=False):\n",
        "\n",
        "    self.kern = kern\n",
        "    self.feature_reduction = feature_reduction\n",
        "    self.scale = Scaled\n",
        "\n",
        "    self.Xtrain = X\n",
        "    self.ytrain = y\n",
        "    \n",
        "    if self.scale:\n",
        "      #Standardise\n",
        "      self.scaler = preprocessing.StandardScaler().fit(self.Xtrain)\n",
        "      self.Xtrain = self.scaler.transform(self.Xtrain)\n",
        "    if self.feature_reduction:\n",
        "      #PCA\n",
        "      self.pca = PCA()\n",
        "      self.Xtrain = self.pca.fit_transform(self.Xtrain,self.ytrain)\n",
        "\n",
        "    self.clf = svm.SVC(kernel=self.kern)\n",
        "    self.clf.fit(self.Xtrain, self.ytrain)\n",
        "\n",
        "    pass\n",
        "\n",
        "  def fit_text(self, X,y,kern='rbf',feature_reduction=False, Scaled=False):\n",
        "    self.kern = kern\n",
        "    self.feature_reduction = feature_reduction\n",
        "    self.scale = Scaled\n",
        "\n",
        "    self.Xtrain = X\n",
        "    self.ytrain = y\n",
        "    \n",
        "    if self.scale:\n",
        "      #Standardise\n",
        "      self.scaler = preprocessing.StandardScaler().fit(self.Xtrain)\n",
        "      self.Xtrain = self.scaler.transform(self.Xtrain)\n",
        "    if self.feature_reduction:\n",
        "      #PCA\n",
        "      self.pca = PCA()\n",
        "      self.Xtrain = self.pca.fit_transform(self.Xtrain,self.ytrain)\n",
        "\n",
        "    self.clf = svm.SVC(kernel=self.kern)\n",
        "    self.clf.fit(self.Xtrain, self.ytrain)\n",
        "    pass\n",
        "\n",
        "  def predict_image(self, X, Transform=True):\n",
        "    # prediction routine for the SVM\n",
        "    if Transform:\n",
        "      if self.scale: X = self.scaler.transform(X)\n",
        "      if self.feature_reduction: X = self.pca.transform(X)\n",
        "    y_pred = self.clf.predict(X)\n",
        "    return y_pred\n",
        "\n",
        "  def predict_text(self, X,Transform=True):\n",
        "    # prediction routine for the SVM\n",
        "    if Transform:\n",
        "      if self.scale: X = self.scaler.transform(X)\n",
        "      if self.feature_reduction: X = self.pca.transform(X)\n",
        "    y_pred = self.clf.predict(X)\n",
        "    return y_pred\n",
        "\n",
        "  \n",
        "  def cross_validation(self,X,y,feature_reduction,scaled,data,kern):\n",
        "\n",
        "    self.kern = kern\n",
        "    self.feature_reduction = feature_reduction\n",
        "    self.scale = scaled\n",
        "    \n",
        "    # prepare cross validation\n",
        "    kfold = KFold(10,True,3)\n",
        "    acc = []\n",
        "    # enumerate splits\n",
        "    if data is 'image':\n",
        "      for train, validate in kfold.split(X):\n",
        "        self.fit_image(X[train],y[train])\n",
        "        pred = self.predict_image(X[validate])\n",
        "        acc.append(accuracy_score(y[validate], pred))\n",
        "    elif data is 'text':\n",
        "      for train, validate in kfold.split(X):\n",
        "        self.fit_text(X[train],y[train])\n",
        "        pred = self.predict_text(X[validate])\n",
        "        acc.append(accuracy_score(y[validate], pred))\n",
        "\n",
        "    return np.mean(acc)"
      ],
      "id": "remarkable-physiology",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVRi9nQlT5yo"
      },
      "source": [
        "### Hyperparameter tuning for SVM Image Classification\n",
        "\n",
        "```\n",
        "from google.colab import files \n",
        "df = pd.DataFrame(columns=['PCA','Scaled','Kernel','Sigma','Accuracy'])\n",
        "counter = 0\n",
        "for kern in reversed(['linear', 'rbf','sigmoid','poly',exp_kernel,laplacian_kernel,rational_quadratic_kernel,log_kernel,spline_kernel,cauchy_kernel]):\n",
        "  for feature_reduction in [1,0]:\n",
        "    for scaled in [1,0]:\n",
        "      for sig in [0.6]:\n",
        "        try:\n",
        "          sv = SVMClassifier()\n",
        "          acc = sv.cross_validation(Xtrain,ytrain,feature_reduction,scaled,'image',kern)\n",
        "          if not isinstance(kern,str):\n",
        "            new_row = {'PCA':feature_reduction,'Scaled':scaled,'Kernel':kern.__name__,'Sigma':sig,'Accuracy':acc}\n",
        "          else: new_row = {'PCA':feature_reduction,'Scaled':scaled,'Kernel':kern,'Sigma':sig,'Accuracy':acc}\n",
        "        except Exception as err:\n",
        "          if not isinstance(kern,str):\n",
        "            new_row = {'PCA':feature_reduction,'Scaled':scaled,'Kernel':kern.__name__,'Sigma':sig,'Accuracy':err}\n",
        "          else: new_row = {'PCA':feature_reduction,'Scaled':scaled,'Kernel':kern,'Sigma':sig,'Accuracy':err}\n",
        " \n",
        "        df = df.append(new_row,ignore_index=True)\n",
        "        counter+=1\n",
        "        if not isinstance(kern,str):\n",
        "          print('Kernel:',kern.__name__)\n",
        "        else:\n",
        "          print('Kernel:',kern)\n",
        "        if counter%4 == 0:\n",
        "          df.to_csv(f\"svmimagetuning.csv\")\n",
        "          \n",
        "```\n",
        "\n"
      ],
      "id": "jVRi9nQlT5yo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flexible-corpus"
      },
      "source": [
        "#### Train the SVM classifer and evaluate it on the train-test split"
      ],
      "id": "flexible-corpus"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "interior-melbourne",
        "outputId": "78d55d8e-e965-46e0-f2ba-18c34dcc36ce"
      },
      "source": [
        "sc = SVMClassifier()\n",
        "sc.fit_image(Xtrain, ytrain,kern=log_kernel,feature_reduction = False, Scaled=False)\n",
        "y_pred = sc.predict_image(Xtest)\n",
        "print('accuracy', accuracy_score(ytest, y_pred))"
      ],
      "id": "interior-melbourne",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.43333333333333335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instant-honor"
      },
      "source": [
        "### Test function"
      ],
      "id": "instant-honor"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liberal-restriction"
      },
      "source": [
        "def test_func_svm_image(image_dataset_train, image_dataset_test):\n",
        "    (X_train, Y_train) = obtain_dataset(image_dataset_train)\n",
        "    (X_test, Y_test) = obtain_dataset(image_dataset_test)\n",
        "    sc = SVMClassifier()\n",
        "    sc.fit_image(X_train, Y_train)\n",
        "    y_pred = sc.predict_image(X_test)\n",
        "    print('accuracy', accuracy_score(Y_test, y_pred))\n",
        "    return y_pred"
      ],
      "id": "liberal-restriction",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reported-genre"
      },
      "source": [
        "# Task 3\n",
        "\n",
        "In this task, you need to obtain sentiment analysis for the provided dataset. The dataset consists of movie reviews with the sentiments being provided. The sentiments are either positive or negative. You need to train a boosting based classifier to obtain train and cross-validate on the dataset provided. The method will be evaluated against an external test set."
      ],
      "id": "reported-genre"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agreed-booking"
      },
      "source": [
        "#### Process the text and obtain a bag of words-based features "
      ],
      "id": "agreed-booking"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3ouN6Tu626D"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "stopwordlist = stopwords.words('english')\n",
        "\n",
        "stemmed_stopwords = [re.sub(r\"[^A-Za-z0-9\\-]\", \" \", porter.stem(word)).split() for word in stopwordlist]\n",
        "\n",
        "for i,words in enumerate(stemmed_stopwords):\n",
        "  if isinstance(words,list): stemmed_stopwords[i]= words[0]\n",
        "\n",
        "def label_y(row):\n",
        "  if row['sentiment'] == 'negative': return 0\n",
        "  if row['sentiment'] == 'positive': return 1\n",
        "\n",
        "\n",
        "def stemming_tokeniser(review):\n",
        "  from nltk.stem import PorterStemmer\n",
        "  porter = PorterStemmer()\n",
        "  import re\n",
        "\n",
        "  words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", review).lower().split()\n",
        "  words = [porter.stem(word) for word in words]\n",
        "  return words\n",
        "\n",
        "\n",
        "def extract_bag_of_words(filename, n_features=1000,stpwrds=stemmed_stopwords):\n",
        "  from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "  df = pd.read_csv(filename)\n",
        "\n",
        "  df['y'] = df.apply(lambda row: label_y(row),axis=1)\n",
        "\n",
        "  xtrainlist = df['review'].tolist()\n",
        "\n",
        "  tfidf_vectorizer = TfidfVectorizer(tokenizer= stemming_tokeniser,max_df=0.95, min_df=3,max_features=n_features,stop_words=stpwrds)\n",
        "\n",
        "  X = tfidf_vectorizer.fit_transform(xtrainlist).toarray()\n",
        "  y = np.array(df['y'])\n",
        "\n",
        "  return (X,y)\n"
      ],
      "id": "e3ouN6Tu626D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exterior-anime",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e0dba05-fe53-4d26-b95a-764db10f77ad"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_file_name='movie_review_train.csv'\n",
        "(X_train, Y_train) = extract_bag_of_words(train_file_name)\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X_train, Y_train, train_size=0.8, random_state=123)"
      ],
      "id": "exterior-anime",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['becau'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FmhajV2V7VF"
      },
      "source": [
        "### Hyperparameter tuning Boosting for text classification \n",
        "\n",
        "```\n",
        "from google.colab import files \n",
        "df = pd.DataFrame(columns=['TreeDepth','SampleSize','No.WeakClassifiers','Accuracy'])\n",
        "counter = 0\n",
        "for n_classifiers in range(5,40,4):\n",
        "  for samples in range(50,1200,100):\n",
        "    for trees in range(1,45,5):\n",
        "      try:\n",
        "        bc = BoostingClassifier()\n",
        "        acc = bc.cross_validation(Xtrain, ytrain,feature_reduction=False,Scaled=False,WeakClassifiers=n_classifiers,SampleSize=samples,TreeDepth=trees)\n",
        "        new_row = {'TreeDepth':trees,'SampleSize':samples,'No.WeakClassifiers':n_classifiers,'Accuracy':acc}\n",
        "      except Exception as err:\n",
        "        new_row = {'TreeDepth':trees,'SampleSize':samples,'No.WeakClassifiers':n_classifiers,'Accuracy':err}\n",
        "      \n",
        "      df = df.append(new_row,ignore_index=True)\n",
        "      counter+=1\n",
        "      if counter%20 == 0:\n",
        "        df.to_csv(f\"textboosting{counter}.csv\")\n",
        "        files.download(f\"textboosting{counter}.csv\")\n",
        "```\n",
        "\n"
      ],
      "id": "0FmhajV2V7VF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk38bINZ5qsH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a775a4-585f-4e4b-acf4-9dd9a824a34f"
      },
      "source": [
        "bc = BoostingClassifier(feature_reduction=False,scaling=False,num_classifiers=40,samplesize=1000,max_tree_depth=40)\n",
        "bc.fit(Xtrain, ytrain)\n",
        "y_pred = bc.predict(Xtest)\n",
        "\n",
        "print('accuracy', accuracy_score(ytest, y_pred))"
      ],
      "id": "sk38bINZ5qsH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orange-state"
      },
      "source": [
        "### Test function "
      ],
      "id": "orange-state"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "satisfactory-thomson"
      },
      "source": [
        "def test_func_boosting_text(text_dataset_train, text_dataset_test):\n",
        "    (X_train, Y_train) = extract_bag_of_words(text_dataset_train)\n",
        "    (X_test, Y_test) = extract_bag_of_words(text_dataset_test)\n",
        "    \n",
        "    bc = BoostingClassifier(feature_reduction = False,scaling=False, samplesize=950,num_classifiers=37,max_tree_depth=41)\n",
        "    bc.fit(X_train, Y_train)\n",
        "    y_pred = bc.predict(X_test)    \n",
        "    print('accuracy', accuracy_score(Y_test, y_pred))\n",
        "    return y_pred"
      ],
      "id": "satisfactory-thomson",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "downtown-niger"
      },
      "source": [
        "# Task 4\n",
        "\n",
        "Classify the above movie review dataset using a Support Vector Machine (SVM)."
      ],
      "id": "downtown-niger"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnGSsPixYpTv"
      },
      "source": [
        "sc = SVMClassifier()\n",
        "acc = sc.cross_validation(Xtrain,ytrain,feature_reduction=True,scaled=False,data='text',kern=log_kernel)\n",
        "print(acc)"
      ],
      "id": "dnGSsPixYpTv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "metropolitan-arrival",
        "outputId": "2ce3db84-0600-40bf-8da9-942cc62f4e2b"
      },
      "source": [
        "# your code for svm based training of the dataset\n",
        "sc = SVMClassifier()\n",
        "sc.fit_text(Xtrain, ytrain,kern=log_kernel)\n",
        "y_pred = sc.predict_text(Xtest)\n",
        "print('accuracy', accuracy_score(ytest, y_pred))"
      ],
      "id": "metropolitan-arrival",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72qhq_xkctbu",
        "outputId": "437be3c9-6eb9-43ac-b69b-562a96ba66ec"
      },
      "source": [
        "sc = SVMClassifier()\n",
        "sc.fit_text(Xtrain, ytrain,kern='linear',feature_reduction=False)\n",
        "y_pred = sc.predict_text(Xtest)\n",
        "print('Linear Kernel Accuracy', accuracy_score(ytest, y_pred))"
      ],
      "id": "72qhq_xkctbu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YjwXhC6c46Y",
        "outputId": "3be8ccf6-ccac-4493-d709-71715984f5c3"
      },
      "source": [
        "sc = SVMClassifier()\n",
        "sc.fit_text(Xtrain, ytrain,kern='rbf',feature_reduction=False)\n",
        "y_pred = sc.predict_text(Xtest)\n",
        "print('RBF Kernel Accuracy', accuracy_score(ytest, y_pred))"
      ],
      "id": "7YjwXhC6c46Y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RBF Kernel Accuracy 0.857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la3yQORFfH0g",
        "outputId": "061616e0-c2c8-4963-cf33-75e7d4762ac8"
      },
      "source": [
        "sc = SVMClassifier()\n",
        "sc.fit_text(Xtrain, ytrain,kern='poly',feature_reduction=False)\n",
        "y_pred = sc.predict_text(Xtest)\n",
        "print('Poly Kernel Accuracy', accuracy_score(ytest, y_pred))"
      ],
      "id": "la3yQORFfH0g",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Poly Kernel Accuracy 0.831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6zfGtxOfUnM",
        "outputId": "52926278-3885-4cb5-a8ab-6acdf4b6ee17"
      },
      "source": [
        "sc = SVMClassifier()\n",
        "sc.fit_text(Xtrain, ytrain,kern='sigmoid',feature_reduction=False)\n",
        "y_pred = sc.predict_text(Xtest)\n",
        "print('Sigmoid Kernel Accuracy', accuracy_score(ytest, y_pred))"
      ],
      "id": "k6zfGtxOfUnM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sigmoid Kernel Accuracy 0.855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO1NJRCOfWvv"
      },
      "source": [
        "sc = SVMClassifier()\n",
        "sc.fit_text(Xtrain, ytrain,kern=laplacian_kernel,feature_reduction=False)\n",
        "y_pred = sc.predict_text(Xtest)\n",
        "print('Laplacian Kernel Accuracy', accuracy_score(ytest, y_pred))"
      ],
      "id": "UO1NJRCOfWvv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pXOXgpdfod8"
      },
      "source": [
        "sc = SVMClassifier()\n",
        "sc.fit_text(Xtrain, ytrain,kern=spline_kernel,feature_reduction=False)\n",
        "y_pred = sc.predict_text(Xtest)\n",
        "print('Spline Kernel Accuracy', accuracy_score(ytest, y_pred))"
      ],
      "id": "1pXOXgpdfod8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VFgSnNiftMG"
      },
      "source": [
        "sc = SVMClassifier()\n",
        "sc.fit_text(Xtrain, ytrain,kern=cauchy_kernel,feature_reduction=False)\n",
        "y_pred = sc.predict_text(Xtest)\n",
        "print('Cauchy Kernel Accuracy', accuracy_score(ytest, y_pred))"
      ],
      "id": "2VFgSnNiftMG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "educated-wallace"
      },
      "source": [
        "### Test function "
      ],
      "id": "educated-wallace"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "behind-skiing"
      },
      "source": [
        "def test_func_svm_text(text_dataset_train, text_dataset_test):\n",
        "    (X_train, Y_train) = extract_bag_of_words(text_dataset_train)\n",
        "    (X_test, Y_test) = extract_bag_of_words(text_dataset_test)\n",
        "    sc = SVMClassifier()\n",
        "    sc.fit_text(X_train, Y_train)\n",
        "    y_pred = sc.predict_text(X_test)\n",
        "    print('accuracy', accuracy_score(Y_test, y_pred))\n",
        "    return y_pred"
      ],
      "id": "behind-skiing",
      "execution_count": null,
      "outputs": []
    }
  ]
}