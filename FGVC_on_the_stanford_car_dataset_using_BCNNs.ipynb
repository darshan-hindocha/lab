{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FGVC on the Stanford Car Dataset using BCNNs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1H6GBA_lxj0lUNC-PrBx1CcuUr04r8AtE",
      "authorship_tag": "ABX9TyOWrOJPqy0EV2W8l0bO0zzV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darshan-hindocha/lab/blob/main/FGVC_on_the_stanford_car_dataset_using_BCNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNRZF7Q4zs2T"
      },
      "source": [
        "# Fine-Grained Visual Classification on the Stanford Car Dataset using Bilinear Convolutional Neural Networks\n",
        "\n",
        "\n",
        "## Instructions for running this notebook\n",
        "\n",
        "Download data from - https://www.kaggle.com/jutrera/stanford-car-dataset-by-classes-folder\n",
        "\n",
        "Upload car_data.zip data to google drive. Alternatively try to upload car_data.zip to drive directly from - https://drive.google.com/file/d/16luuHDPxI2c9EhV2QvZKsKOeZyk9PPC_/view?usp=sharing \n",
        "\n",
        "Upload 'anno_test.csv', 'anno_train.csv' and 'names.csv' to colab temporary file directory. [files found here https://www.kaggle.com/jutrera/stanford-car-dataset-by-classes-folder ]\n",
        "\n",
        "Mount Drive to Colab.\n",
        "\n",
        "Copy path of 'car_data.zip' to image_folder_name variable below (might not need to change)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glXUspXs01_9"
      },
      "source": [
        "image_folder_name = '/content/drive/MyDrive/car_data.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMRUj0DN1s_j"
      },
      "source": [
        "For neatness we collapse some cells as below. Double click the cell to see the contents. Make sure to run these cells also"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "vZsBHqQ16MXj"
      },
      "source": [
        "#@title Imports\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from numpy import newaxis\n",
        "from google.colab import files\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.layers import Lambda, Reshape, Activation,Dense,Concatenate,Dropout\n",
        "from keras.models import Model, Input, load_model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications import ResNet101\n",
        "\n",
        "from keras.initializers import glorot_normal\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from time import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_UiMIEoLcAr",
        "outputId": "0e3ebc10-c124-4da0-e79d-75def4d1576b"
      },
      "source": [
        "#upload the image dataset as 'image_dataset.zip' or alternatively rename the string below\n",
        "\n",
        "from zipfile import ZipFile\n",
        "\n",
        "file_name = image_folder_name\n",
        "\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDpb2YRW1QCV"
      },
      "source": [
        "Ensure that after unzipping, the car_data folder is in the following path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toWO-irmLZ9X"
      },
      "source": [
        "image_folder = '/content/car_data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Sv-rgCdi2yY"
      },
      "source": [
        "num_classes = len(glob.glob(image_folder + '/train/*'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q9czfwM2au4"
      },
      "source": [
        "## Pre-Processing (bounding boxes)\n",
        "\n",
        "The data comes with bounding boxes that we apply to crop the images. Basic reshaping also done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "hsu-oJSiLePr"
      },
      "source": [
        "#@title Read bounding information CSVs\n",
        "\n",
        "df_bound_boxes_train = pd.read_csv('anno_train.csv',header=None)\n",
        "df_bound_boxes_test = pd.read_csv('anno_test.csv',header=None)\n",
        "df_names = pd.read_csv('names.csv',header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "MSanR-aKLgL4"
      },
      "source": [
        "#@title Make folders for bounded images\n",
        "\n",
        "os.mkdir('/content/car_data_bounded/')\n",
        "os.mkdir('/content/car_data_bounded/train')\n",
        "os.mkdir('/content/car_data_bounded/test')\n",
        "\n",
        "for class_label in glob.glob(image_folder + '/train/*'):\n",
        "\n",
        "  os.mkdir('/content/car_data_bounded/train/' + class_label.split('/')[-1])\n",
        "  os.mkdir('/content/car_data_bounded/test/' + class_label.split('/')[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuboJ7LuLjT0"
      },
      "source": [
        "## Crop and store bounded images\n",
        "\n",
        "for t in ['train','test']:\n",
        "\n",
        "  for file_name in glob.glob('car_data/'+t+'/*/*'):\n",
        "\n",
        "    # extracting the row from the csv that has the details about the image in each iteration\n",
        "    if t is 'train':\n",
        "      filter = df_bound_boxes_train[0] == file_name.split('/')[-1]\n",
        "      file_details = np.array(df_bound_boxes_train.loc[filter])\n",
        "    else:\n",
        "      filter = df_bound_boxes_test[0] == file_name.split('/')[-1]\n",
        "      file_details = np.array(df_bound_boxes_test.loc[filter])\n",
        "    \n",
        "    _,x1,y1,x2,y2,file_class = file_details[0]  \n",
        "\n",
        "    image = cv2.imread(file_name)\n",
        "    height,width = image.shape[:2]\n",
        "    \n",
        "    crop_image = image[y1:y2, x1:x2]\n",
        "\n",
        "    cv2.imwrite('/content/car_data_bounded/'+t+'/'+file_name.split('/')[-2]+'/'+file_name.split('/')[-1], crop_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhID6Dbp26t0"
      },
      "source": [
        "## Data Augmentation\n",
        "\n",
        "Using the Keras ImageDataGenerator Tool"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYPzx4l9K7XA",
        "outputId": "8b0de4d9-187a-4a55-8cfb-312a09550133"
      },
      "source": [
        "## ImageDataGenerator uses file path to obtain the class of images\n",
        "\n",
        "base_dir = 'car_data_bounded'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=10,\n",
        "    shear_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "\n",
        "## Validation Data is NOT augmented\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "## Flow training images in batches of 32 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size= 32)\n",
        "        #,save_to_dir='augmented_data')\n",
        "\n",
        "## Flow validation images in batches of 32 using val_datagen generator\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size = 32)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8144 images belonging to 196 classes.\n",
            "Found 8041 images belonging to 196 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "mO1vmy7h-wIS"
      },
      "source": [
        "#@title Keras Bilinear Layers\n",
        "\n",
        "\n",
        "## We use the following Layer operations from: https://github.com/ryanfwy/BCNN-keras-clean\n",
        "def _outer_product(x):\n",
        "    return K.batch_dot(x[0], x[1], axes=[1, 1]) / x[0].get_shape().as_list()[1]\n",
        "\n",
        "def _signed_sqrt(x):\n",
        "    return K.sign(x) * K.sqrt(K.abs(x) + 1e-9)\n",
        "\n",
        "def _l2_normalize(x, axis=-1):\n",
        "    return K.l2_normalize(x, axis=axis)\n",
        "\n",
        "\n",
        "\n",
        "## Our bilinear layer - not used\n",
        "def bilinear_layer(inputs):\n",
        "    x1,x2 = inputs\n",
        "    #x1  = input from stream A, with shape (batchsize,height,width,channel)\n",
        "    #x2  = input from stream B, with shape (batchsize,height,width,channel)\n",
        "\n",
        "    #if you want to print the shape of this\n",
        "    #print(np.einsum('bmc,bnc->bmn',x1.reshape(2,289,64),x2.reshape(2,289,64)).reshape(2,83521).shape)\n",
        "    batch_size = x1.shape[0]\n",
        "    channels = x1.shape[3]\n",
        "    h,w = x1.shape[1],x1.shape[2]\n",
        "    h_w = h*w\n",
        "    out = tf.einsum('bmc,bnc->bmn',K.reshape(x1,(batch_size,h_w,channels)),K.reshape(x2,(batch_size,h_w,channels)))\n",
        "\n",
        "    return K.reshape(out,(batch_size,h_w*h_w))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWbKeRJS45WP"
      },
      "source": [
        "## BCNN-VGG16 (196 classes) original implementation with transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hgeFm3a5v_E"
      },
      "source": [
        "## function to create model\n",
        "\n",
        "def create_model():\n",
        "  \n",
        "  # Our input feature map is 150x150x3: 150x150 for the image pixels, and 3 for\n",
        "  # the three color channels: R, G, and B\n",
        "  img_input1 = Input(shape=[150, 150, 3])\n",
        "\n",
        "  ## Stream A - Transfer Learning\n",
        "  stream_A = VGG16(input_tensor=img_input1,include_top=False,weights='imagenet')\n",
        "\n",
        "  ## Fix layers\n",
        "  for l in stream_A.layers:\n",
        "    l.trainable = False\n",
        "    l._name = l._name + '1'\n",
        "\n",
        "  output_stream_A = stream_A.layers[18].output\n",
        "  A_out_shape = stream_A.layers[18].output_shape\n",
        "  output_stream_A = Reshape([A_out_shape[1]*A_out_shape[2],A_out_shape[3]])(output_stream_A)\n",
        "\n",
        "\n",
        "  ## Stream B - Transfer Learning\n",
        "  stream_B = VGG16(input_tensor = img_input1,include_top=False,weights='imagenet')\n",
        "  for l in stream_B.layers:\n",
        "    l.trainable = False\n",
        "\n",
        "  \n",
        "  output_stream_B = stream_B.layers[18].output\n",
        "  B_out_shape = stream_B.layers[18].output_shape\n",
        "  output_stream_B = Reshape([B_out_shape[1]*B_out_shape[2],B_out_shape[3]])(output_stream_B)\n",
        "\n",
        "\n",
        "  ## Bilinear layer\n",
        "  x = Lambda(_outer_product)([output_stream_A,output_stream_B])\n",
        "  x = Reshape([A_out_shape[-1]*B_out_shape[-1]])(x)\n",
        "\n",
        "  # Signed square-root Layer\n",
        "  x = Lambda(_signed_sqrt)(x)\n",
        "  # L2 normalization Layer\n",
        "  x = Lambda(_l2_normalize)(x)\n",
        "\n",
        "  # Create a fully connected layer with ReLU activation and 196 hidden units\n",
        "  x = Dense(num_classes,kernel_initializer=glorot_normal())(x)\n",
        "  output = Activation('softmax')(x)\n",
        "\n",
        "  bcnn_model = Model( img_input1, output)\n",
        "\n",
        "  ## Configure and compile the model\n",
        "  bcnn_model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "  return bcnn_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYN1WEpkCx32",
        "outputId": "87722ccb-83d8-4f58-cd80-ece35d41e1db"
      },
      "source": [
        "bcnn_model = create_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKPvDYUX5joq"
      },
      "source": [
        "Running the training in 10 epoch steps so that if the runtime collapse we can save our results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B5dYyao0iI2"
      },
      "source": [
        "def run_training():\n",
        "  for group in range(10):\n",
        "    print('Training Group: ',group)\n",
        "    history = bcnn_model.fit(train_generator,epochs=10,validation_data=validation_generator,verbose=2)\n",
        "   \n",
        "    hist_df = pd.DataFrame(history.history)\n",
        "    hist_df.to_csv(f'/content/drive/MyDrive/history{group}.csv')\n",
        "\n",
        "    bcnn_model.save('/content/drive/MyDrive/bcnn_model.h5', overwrite=True)\n",
        "  pass\n",
        "run_training()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QML8xBv7AZA"
      },
      "source": [
        "## BCNN-VGG16 (196 classes) w/ Dropout Layer + Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfohWb4J6_zP"
      },
      "source": [
        "def create_v2_model():\n",
        "  \n",
        "  # Our input feature map is 150x150x3: 150x150 for the image pixels, and 3 for\n",
        "  # the three color channels: R, G, and B\n",
        "  img_input1 = Input(shape=[150, 150, 3])\n",
        "\n",
        "  ## Stream A - Transfer Learning\n",
        "  stream_A = VGG16(input_tensor=img_input1,include_top=False,weights='imagenet')\n",
        "\n",
        "  ## Fix layers\n",
        "  for l in stream_A.layers:\n",
        "    l.trainable = False\n",
        "    l._name = l._name + '1'\n",
        "\n",
        "  output_stream_A = stream_A.layers[18].output\n",
        "  A_out_shape = stream_A.layers[18].output_shape\n",
        "  output_stream_A = Reshape([A_out_shape[1]*A_out_shape[2],A_out_shape[3]])(output_stream_A)\n",
        "\n",
        "\n",
        "  ## Stream B - Transfer Learning\n",
        "  stream_B = VGG16(input_tensor = img_input1,include_top=False,weights='imagenet')\n",
        "  for l in stream_B.layers:\n",
        "    l.trainable = False\n",
        "\n",
        "  \n",
        "  output_stream_B = stream_B.layers[18].output\n",
        "  B_out_shape = stream_B.layers[18].output_shape\n",
        "  output_stream_B = Reshape([B_out_shape[1]*B_out_shape[2],B_out_shape[3]])(output_stream_B)\n",
        "\n",
        "\n",
        "  ## Bilinear layer\n",
        "  x = Lambda(_outer_product)([output_stream_A,output_stream_B])\n",
        "  x = Reshape([A_out_shape[-1]*B_out_shape[-1]])(x)\n",
        "\n",
        "  # Signed square-root Layer\n",
        "  x = Lambda(_signed_sqrt)(x)\n",
        "  # L2 normalization Layer\n",
        "  x = Lambda(_l2_normalize)(x)\n",
        "  \n",
        "  x = Dropout(0.5)(x)\n",
        "\n",
        "  x = Dense(num_classes,kernel_initializer=glorot_normal())(x)\n",
        "  output = Activation('softmax')(x)\n",
        "\n",
        "  bcnn_v2_model = Model( img_input1, output)\n",
        "\n",
        "  # Configure and compile the model\n",
        "  bcnn_v2_model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  return bcnn_v2_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOclZdJ56rg5"
      },
      "source": [
        "\n",
        "We tried this but considered it better to re-run the new model from scratch\n",
        "```\n",
        "model_overfit = load_model('/content/drive/MyDrive/bcnn_model.h5')\n",
        "\n",
        "overfit_weights = [layer.get_weights() for layer in model_overfit.layers]\n",
        "  \n",
        "for i in range(len(overfit_weights)-2):\n",
        "  bcnn_v2_model.layers[i].set_weights(overfit_weights[i])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p1IDa-lJ6un"
      },
      "source": [
        "bcnn_v2_model = create_v2_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8lHRxm_IW8l",
        "outputId": "fdac0885-9a48-47e4-9ac5-e05b5f43ade1"
      },
      "source": [
        "def run_training():\n",
        "  for group in range(10):\n",
        "    print('Training Group: ',group)\n",
        "    history = bcnn_v2_model.fit(train_generator,epochs=10,validation_data=validation_generator,verbose=2)\n",
        "   \n",
        "    hist_df = pd.DataFrame(history.history)\n",
        "    hist_df.to_csv(f'/content/drive/MyDrive/history_v2_{group}.csv')\n",
        "\n",
        "    \n",
        "    bcnn_v2_model.save('/content/drive/MyDrive/bcnn_v2_model.h5', overwrite=True)\n",
        "  pass\n",
        "run_training()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Group:  0\n",
            "Epoch 1/10\n",
            "255/255 - 100s - loss: 0.5342 - accuracy: 0.9584 - val_loss: 2.0724 - val_accuracy: 0.5099\n",
            "Epoch 2/10\n",
            "255/255 - 100s - loss: 0.5161 - accuracy: 0.9605 - val_loss: 2.0572 - val_accuracy: 0.5136\n",
            "Epoch 3/10\n",
            "255/255 - 100s - loss: 0.4877 - accuracy: 0.9629 - val_loss: 2.0473 - val_accuracy: 0.5115\n",
            "Epoch 4/10\n",
            "255/255 - 100s - loss: 0.4658 - accuracy: 0.9683 - val_loss: 2.0291 - val_accuracy: 0.5155\n",
            "Epoch 5/10\n",
            "255/255 - 100s - loss: 0.4373 - accuracy: 0.9732 - val_loss: 2.0294 - val_accuracy: 0.5192\n",
            "Epoch 6/10\n",
            "255/255 - 99s - loss: 0.4248 - accuracy: 0.9708 - val_loss: 2.0289 - val_accuracy: 0.5195\n",
            "Epoch 7/10\n",
            "255/255 - 100s - loss: 0.3988 - accuracy: 0.9727 - val_loss: 2.0200 - val_accuracy: 0.5186\n",
            "Epoch 8/10\n",
            "255/255 - 101s - loss: 0.3811 - accuracy: 0.9742 - val_loss: 1.9868 - val_accuracy: 0.5248\n",
            "Epoch 9/10\n",
            "255/255 - 101s - loss: 0.3622 - accuracy: 0.9786 - val_loss: 1.9929 - val_accuracy: 0.5237\n",
            "Epoch 10/10\n",
            "255/255 - 100s - loss: 0.3519 - accuracy: 0.9802 - val_loss: 1.9910 - val_accuracy: 0.5246\n",
            "Training Group:  1\n",
            "Epoch 1/10\n",
            "255/255 - 100s - loss: 0.3400 - accuracy: 0.9811 - val_loss: 1.9695 - val_accuracy: 0.5258\n",
            "Epoch 2/10\n",
            "255/255 - 100s - loss: 0.3219 - accuracy: 0.9821 - val_loss: 1.9700 - val_accuracy: 0.5275\n",
            "Epoch 3/10\n",
            "255/255 - 99s - loss: 0.3046 - accuracy: 0.9850 - val_loss: 1.9717 - val_accuracy: 0.5257\n",
            "Epoch 4/10\n",
            "255/255 - 100s - loss: 0.2936 - accuracy: 0.9840 - val_loss: 1.9599 - val_accuracy: 0.5289\n",
            "Epoch 5/10\n",
            "255/255 - 99s - loss: 0.2758 - accuracy: 0.9874 - val_loss: 1.9609 - val_accuracy: 0.5274\n",
            "Epoch 6/10\n",
            "255/255 - 99s - loss: 0.2682 - accuracy: 0.9886 - val_loss: 1.9531 - val_accuracy: 0.5287\n",
            "Epoch 7/10\n",
            "255/255 - 100s - loss: 0.2549 - accuracy: 0.9886 - val_loss: 1.9295 - val_accuracy: 0.5354\n",
            "Epoch 8/10\n",
            "255/255 - 99s - loss: 0.2462 - accuracy: 0.9902 - val_loss: 1.9469 - val_accuracy: 0.5273\n",
            "Epoch 9/10\n",
            "255/255 - 100s - loss: 0.2260 - accuracy: 0.9913 - val_loss: 1.9291 - val_accuracy: 0.5372\n",
            "Epoch 10/10\n",
            "255/255 - 99s - loss: 0.2217 - accuracy: 0.9915 - val_loss: 1.9423 - val_accuracy: 0.5312\n",
            "Training Group:  2\n",
            "Epoch 1/10\n",
            "255/255 - 102s - loss: 0.2146 - accuracy: 0.9918 - val_loss: 1.9306 - val_accuracy: 0.5345\n",
            "Epoch 2/10\n",
            "255/255 - 99s - loss: 0.2072 - accuracy: 0.9931 - val_loss: 1.9233 - val_accuracy: 0.5369\n",
            "Epoch 3/10\n",
            "255/255 - 99s - loss: 0.1972 - accuracy: 0.9929 - val_loss: 1.9185 - val_accuracy: 0.5363\n",
            "Epoch 4/10\n",
            "255/255 - 99s - loss: 0.1876 - accuracy: 0.9948 - val_loss: 1.9207 - val_accuracy: 0.5349\n",
            "Epoch 5/10\n",
            "255/255 - 100s - loss: 0.1768 - accuracy: 0.9939 - val_loss: 1.9235 - val_accuracy: 0.5331\n",
            "Epoch 6/10\n",
            "255/255 - 99s - loss: 0.1744 - accuracy: 0.9936 - val_loss: 1.9157 - val_accuracy: 0.5321\n",
            "Epoch 7/10\n",
            "255/255 - 99s - loss: 0.1611 - accuracy: 0.9947 - val_loss: 1.9068 - val_accuracy: 0.5370\n",
            "Epoch 8/10\n",
            "255/255 - 100s - loss: 0.1606 - accuracy: 0.9941 - val_loss: 1.8995 - val_accuracy: 0.5418\n",
            "Epoch 9/10\n",
            "255/255 - 100s - loss: 0.1498 - accuracy: 0.9957 - val_loss: 1.8991 - val_accuracy: 0.5404\n",
            "Epoch 10/10\n",
            "255/255 - 99s - loss: 0.1492 - accuracy: 0.9942 - val_loss: 1.8841 - val_accuracy: 0.5448\n",
            "Training Group:  3\n",
            "Epoch 1/10\n",
            "255/255 - 100s - loss: 0.1424 - accuracy: 0.9952 - val_loss: 1.8911 - val_accuracy: 0.5397\n",
            "Epoch 2/10\n",
            "255/255 - 99s - loss: 0.1351 - accuracy: 0.9957 - val_loss: 1.8915 - val_accuracy: 0.5414\n",
            "Epoch 3/10\n",
            "255/255 - 99s - loss: 0.1305 - accuracy: 0.9961 - val_loss: 1.8912 - val_accuracy: 0.5421\n",
            "Epoch 4/10\n",
            "255/255 - 99s - loss: 0.1245 - accuracy: 0.9961 - val_loss: 1.8986 - val_accuracy: 0.5394\n",
            "Epoch 5/10\n",
            "255/255 - 99s - loss: 0.1228 - accuracy: 0.9961 - val_loss: 1.8853 - val_accuracy: 0.5438\n",
            "Epoch 6/10\n",
            "255/255 - 99s - loss: 0.1138 - accuracy: 0.9971 - val_loss: 1.8913 - val_accuracy: 0.5452\n",
            "Epoch 7/10\n",
            "255/255 - 99s - loss: 0.1095 - accuracy: 0.9972 - val_loss: 1.8984 - val_accuracy: 0.5404\n",
            "Epoch 8/10\n",
            "255/255 - 99s - loss: 0.1050 - accuracy: 0.9966 - val_loss: 1.8864 - val_accuracy: 0.5453\n",
            "Epoch 9/10\n",
            "255/255 - 99s - loss: 0.1014 - accuracy: 0.9975 - val_loss: 1.8860 - val_accuracy: 0.5422\n",
            "Epoch 10/10\n",
            "255/255 - 99s - loss: 0.0997 - accuracy: 0.9967 - val_loss: 1.8868 - val_accuracy: 0.5422\n",
            "Training Group:  4\n",
            "Epoch 1/10\n",
            "255/255 - 101s - loss: 0.0975 - accuracy: 0.9968 - val_loss: 1.8794 - val_accuracy: 0.5471\n",
            "Epoch 2/10\n",
            "255/255 - 99s - loss: 0.0942 - accuracy: 0.9974 - val_loss: 1.8809 - val_accuracy: 0.5416\n",
            "Epoch 3/10\n",
            "255/255 - 100s - loss: 0.0890 - accuracy: 0.9971 - val_loss: 1.8769 - val_accuracy: 0.5464\n",
            "Epoch 4/10\n",
            "255/255 - 99s - loss: 0.0864 - accuracy: 0.9978 - val_loss: 1.8807 - val_accuracy: 0.5450\n",
            "Epoch 5/10\n",
            "255/255 - 99s - loss: 0.0839 - accuracy: 0.9961 - val_loss: 1.8697 - val_accuracy: 0.5436\n",
            "Epoch 6/10\n",
            "255/255 - 100s - loss: 0.0794 - accuracy: 0.9972 - val_loss: 1.8703 - val_accuracy: 0.5493\n",
            "Epoch 7/10\n",
            "255/255 - 100s - loss: 0.0801 - accuracy: 0.9969 - val_loss: 1.8798 - val_accuracy: 0.5455\n",
            "Epoch 8/10\n",
            "255/255 - 100s - loss: 0.0747 - accuracy: 0.9971 - val_loss: 1.8756 - val_accuracy: 0.5474\n",
            "Epoch 9/10\n",
            "255/255 - 100s - loss: 0.0717 - accuracy: 0.9979 - val_loss: 1.8595 - val_accuracy: 0.5519\n",
            "Epoch 10/10\n",
            "255/255 - 100s - loss: 0.0723 - accuracy: 0.9973 - val_loss: 1.8676 - val_accuracy: 0.5482\n",
            "Training Group:  5\n",
            "Epoch 1/10\n",
            "255/255 - 101s - loss: 0.0694 - accuracy: 0.9973 - val_loss: 1.8605 - val_accuracy: 0.5538\n",
            "Epoch 2/10\n",
            "255/255 - 99s - loss: 0.0662 - accuracy: 0.9982 - val_loss: 1.8581 - val_accuracy: 0.5504\n",
            "Epoch 3/10\n",
            "255/255 - 100s - loss: 0.0656 - accuracy: 0.9973 - val_loss: 1.8601 - val_accuracy: 0.5488\n",
            "Epoch 4/10\n",
            "255/255 - 99s - loss: 0.0624 - accuracy: 0.9974 - val_loss: 1.8652 - val_accuracy: 0.5520\n",
            "Epoch 5/10\n",
            "255/255 - 99s - loss: 0.0603 - accuracy: 0.9982 - val_loss: 1.8746 - val_accuracy: 0.5462\n",
            "Epoch 6/10\n",
            "255/255 - 99s - loss: 0.0593 - accuracy: 0.9974 - val_loss: 1.8655 - val_accuracy: 0.5512\n",
            "Epoch 7/10\n",
            "255/255 - 99s - loss: 0.0563 - accuracy: 0.9977 - val_loss: 1.8605 - val_accuracy: 0.5518\n",
            "Epoch 8/10\n",
            "255/255 - 100s - loss: 0.0579 - accuracy: 0.9972 - val_loss: 1.8744 - val_accuracy: 0.5463\n",
            "Epoch 9/10\n",
            "255/255 - 99s - loss: 0.0539 - accuracy: 0.9979 - val_loss: 1.8627 - val_accuracy: 0.5501\n",
            "Epoch 10/10\n",
            "255/255 - 100s - loss: 0.0507 - accuracy: 0.9975 - val_loss: 1.8589 - val_accuracy: 0.5520\n",
            "Training Group:  6\n",
            "Epoch 1/10\n",
            "255/255 - 100s - loss: 0.0510 - accuracy: 0.9974 - val_loss: 1.8727 - val_accuracy: 0.5515\n",
            "Epoch 2/10\n",
            "255/255 - 99s - loss: 0.0498 - accuracy: 0.9977 - val_loss: 1.8530 - val_accuracy: 0.5570\n",
            "Epoch 3/10\n",
            "255/255 - 99s - loss: 0.0475 - accuracy: 0.9980 - val_loss: 1.8540 - val_accuracy: 0.5559\n",
            "Epoch 4/10\n",
            "255/255 - 99s - loss: 0.0464 - accuracy: 0.9977 - val_loss: 1.8629 - val_accuracy: 0.5513\n",
            "Epoch 5/10\n",
            "255/255 - 100s - loss: 0.0455 - accuracy: 0.9977 - val_loss: 1.8557 - val_accuracy: 0.5575\n",
            "Epoch 6/10\n",
            "255/255 - 100s - loss: 0.0436 - accuracy: 0.9973 - val_loss: 1.8700 - val_accuracy: 0.5497\n",
            "Epoch 7/10\n",
            "255/255 - 100s - loss: 0.0416 - accuracy: 0.9980 - val_loss: 1.8797 - val_accuracy: 0.5473\n",
            "Epoch 8/10\n",
            "255/255 - 100s - loss: 0.0427 - accuracy: 0.9973 - val_loss: 1.8602 - val_accuracy: 0.5557\n",
            "Epoch 9/10\n",
            "255/255 - 100s - loss: 0.0404 - accuracy: 0.9980 - val_loss: 1.8613 - val_accuracy: 0.5564\n",
            "Epoch 10/10\n",
            "255/255 - 99s - loss: 0.0389 - accuracy: 0.9983 - val_loss: 1.8534 - val_accuracy: 0.5554\n",
            "Training Group:  7\n",
            "Epoch 1/10\n",
            "255/255 - 100s - loss: 0.0392 - accuracy: 0.9975 - val_loss: 1.8589 - val_accuracy: 0.5575\n",
            "Epoch 2/10\n",
            "255/255 - 99s - loss: 0.0362 - accuracy: 0.9982 - val_loss: 1.8665 - val_accuracy: 0.5533\n",
            "Epoch 3/10\n",
            "255/255 - 99s - loss: 0.0362 - accuracy: 0.9979 - val_loss: 1.8458 - val_accuracy: 0.5573\n",
            "Epoch 4/10\n",
            "255/255 - 99s - loss: 0.0352 - accuracy: 0.9977 - val_loss: 1.8473 - val_accuracy: 0.5584\n",
            "Epoch 5/10\n",
            "255/255 - 99s - loss: 0.0346 - accuracy: 0.9978 - val_loss: 1.8536 - val_accuracy: 0.5581\n",
            "Epoch 6/10\n",
            "255/255 - 99s - loss: 0.0334 - accuracy: 0.9983 - val_loss: 1.8512 - val_accuracy: 0.5590\n",
            "Epoch 7/10\n",
            "255/255 - 99s - loss: 0.0331 - accuracy: 0.9978 - val_loss: 1.8770 - val_accuracy: 0.5552\n",
            "Epoch 8/10\n",
            "255/255 - 99s - loss: 0.0325 - accuracy: 0.9975 - val_loss: 1.8566 - val_accuracy: 0.5610\n",
            "Epoch 9/10\n",
            "255/255 - 99s - loss: 0.0308 - accuracy: 0.9978 - val_loss: 1.8497 - val_accuracy: 0.5574\n",
            "Epoch 10/10\n",
            "255/255 - 99s - loss: 0.0302 - accuracy: 0.9983 - val_loss: 1.8704 - val_accuracy: 0.5561\n",
            "Training Group:  8\n",
            "Epoch 1/10\n",
            "255/255 - 100s - loss: 0.0296 - accuracy: 0.9975 - val_loss: 1.8462 - val_accuracy: 0.5620\n",
            "Epoch 2/10\n",
            "255/255 - 99s - loss: 0.0298 - accuracy: 0.9973 - val_loss: 1.8631 - val_accuracy: 0.5581\n",
            "Epoch 3/10\n",
            "255/255 - 99s - loss: 0.0289 - accuracy: 0.9982 - val_loss: 1.8698 - val_accuracy: 0.5568\n",
            "Epoch 4/10\n",
            "255/255 - 99s - loss: 0.0277 - accuracy: 0.9982 - val_loss: 1.8648 - val_accuracy: 0.5596\n",
            "Epoch 5/10\n",
            "255/255 - 100s - loss: 0.0268 - accuracy: 0.9986 - val_loss: 1.8622 - val_accuracy: 0.5615\n",
            "Epoch 6/10\n",
            "255/255 - 99s - loss: 0.0265 - accuracy: 0.9978 - val_loss: 1.8536 - val_accuracy: 0.5616\n",
            "Epoch 7/10\n",
            "255/255 - 99s - loss: 0.0259 - accuracy: 0.9983 - val_loss: 1.8596 - val_accuracy: 0.5624\n",
            "Epoch 8/10\n",
            "255/255 - 99s - loss: 0.0258 - accuracy: 0.9980 - val_loss: 1.8396 - val_accuracy: 0.5656\n",
            "Epoch 9/10\n",
            "255/255 - 99s - loss: 0.0241 - accuracy: 0.9984 - val_loss: 1.8560 - val_accuracy: 0.5644\n",
            "Epoch 10/10\n",
            "255/255 - 99s - loss: 0.0251 - accuracy: 0.9975 - val_loss: 1.8763 - val_accuracy: 0.5552\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZyIz18qHMCf"
      },
      "source": [
        "## BCNN-ResNet101 (196 classes) w/ Dropout Layer + transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HR5tcQsHKNX"
      },
      "source": [
        "def create_resnet_model():\n",
        "  # Our input feature map is 150x150x3: 150x150 for the image pixels, and 3 for\n",
        "  # the three color channels: R, G, and B\n",
        "  img_input1 = Input(shape=[150, 150, 3])\n",
        "\n",
        "  ##Transfer Learning Stream A\n",
        "  stream_A = ResNet101(include_top=False,weights=\"imagenet\",input_tensor=img_input1)\n",
        "\n",
        "  ## We don't fix the weights -  we change the layer names so it doesn't clash with other stream\n",
        "  for l in stream_A.layers:\n",
        "    l._name = l._name + '1'\n",
        "\n",
        "  output_stream_A = stream_A.layers[18].output\n",
        "  A_out_shape = stream_A.layers[18].output_shape\n",
        "\n",
        "  output_stream_A = Reshape([A_out_shape[1]*A_out_shape[2],A_out_shape[3]])(output_stream_A)\n",
        "\n",
        "\n",
        "  ## Transfer Learning Stream B\n",
        "  stream_B = ResNet101(include_top=False,weights=\"imagenet\",input_tensor=img_input1)\n",
        "\n",
        "  output_stream_B = stream_B.layers[18].output\n",
        "  B_out_shape = stream_B.layers[18].output_shape\n",
        "\n",
        "  output_stream_B = Reshape([B_out_shape[1]*B_out_shape[2],B_out_shape[3]])(output_stream_B)\n",
        "\n",
        "  ## Bilinear Layer\n",
        "  x = Lambda(_outer_product)([output_stream_A,output_stream_B])\n",
        "  x = Reshape([A_out_shape[-1]*B_out_shape[-1]])(x)\n",
        "  # Signed square-root\n",
        "  x = Lambda(_signed_sqrt)(x)\n",
        "  # L2 normalization\n",
        "  x = Lambda(_l2_normalize)(x)\n",
        "  \n",
        "  x = Dropout(0.5)(x)\n",
        "  x = Dense(num_classes,kernel_initializer=glorot_normal())(x)\n",
        "  output = Activation('softmax')(x)\n",
        "  \n",
        "  bcnn_resnet_model = Model(img_input1, output)\n",
        "\n",
        "  # Configure and compile the model\n",
        "\n",
        "  bcnn_resnet_model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return bcnn_resnet_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI-5DbZxISZx"
      },
      "source": [
        "bcnn_resnet_model = create_resnet_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBap5VmFJGNi",
        "outputId": "7263044e-4faf-4ed6-ddba-5dca5e1455db"
      },
      "source": [
        "def run_training():\n",
        "  for group in range(10):\n",
        "    print('Training Group: ',group)\n",
        "    history = bcnn_resnet_model.fit(train_generator,epochs=10,validation_data=validation_generator,verbose=2)\n",
        "   \n",
        "    hist_df = pd.DataFrame(history.history)\n",
        "    hist_df.to_csv(f'/content/drive/MyDrive/history_resnet_{group}.csv')\n",
        "\n",
        "    \n",
        "    bcnn_resnet_model.save_weights('/content/drive/MyDrive/bcnn_resnet_modelweights.h5',overwrite=True)\n",
        "  pass\n",
        "run_training()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Group:  0\n",
            "Epoch 1/10\n",
            "255/255 - 98s - loss: 3.2011 - accuracy: 0.2933 - val_loss: 4.1136 - val_accuracy: 0.1420\n",
            "Epoch 2/10\n",
            "255/255 - 97s - loss: 3.0080 - accuracy: 0.3298 - val_loss: 4.2364 - val_accuracy: 0.1415\n",
            "Epoch 3/10\n",
            "255/255 - 97s - loss: 2.8412 - accuracy: 0.3691 - val_loss: 3.6135 - val_accuracy: 0.2148\n",
            "Epoch 4/10\n",
            "255/255 - 97s - loss: 2.6759 - accuracy: 0.3997 - val_loss: 4.0627 - val_accuracy: 0.1604\n",
            "Epoch 5/10\n",
            "255/255 - 97s - loss: 2.5249 - accuracy: 0.4447 - val_loss: 3.7064 - val_accuracy: 0.2110\n",
            "Epoch 6/10\n",
            "255/255 - 97s - loss: 2.3747 - accuracy: 0.4711 - val_loss: 3.6787 - val_accuracy: 0.1969\n",
            "Epoch 7/10\n",
            "255/255 - 97s - loss: 2.2159 - accuracy: 0.5183 - val_loss: 3.5206 - val_accuracy: 0.2450\n",
            "Epoch 8/10\n",
            "255/255 - 97s - loss: 2.1065 - accuracy: 0.5379 - val_loss: 3.6415 - val_accuracy: 0.2247\n",
            "Epoch 9/10\n",
            "255/255 - 97s - loss: 1.9763 - accuracy: 0.5699 - val_loss: 3.3521 - val_accuracy: 0.2720\n",
            "Epoch 10/10\n",
            "255/255 - 98s - loss: 1.8648 - accuracy: 0.5961 - val_loss: 3.6158 - val_accuracy: 0.2322\n",
            "Training Group:  1\n",
            "Epoch 1/10\n",
            "255/255 - 97s - loss: 1.7443 - accuracy: 0.6273 - val_loss: 3.3267 - val_accuracy: 0.2789\n",
            "Epoch 2/10\n",
            "255/255 - 97s - loss: 1.6364 - accuracy: 0.6548 - val_loss: 3.0452 - val_accuracy: 0.3327\n",
            "Epoch 3/10\n",
            "255/255 - 97s - loss: 1.5410 - accuracy: 0.6762 - val_loss: 3.1961 - val_accuracy: 0.3073\n",
            "Epoch 4/10\n",
            "255/255 - 97s - loss: 1.4430 - accuracy: 0.7025 - val_loss: 3.2255 - val_accuracy: 0.3049\n",
            "Epoch 5/10\n",
            "255/255 - 97s - loss: 1.3592 - accuracy: 0.7231 - val_loss: 3.1100 - val_accuracy: 0.3251\n",
            "Epoch 6/10\n",
            "255/255 - 97s - loss: 1.2641 - accuracy: 0.7504 - val_loss: 3.6736 - val_accuracy: 0.2375\n",
            "Epoch 7/10\n",
            "255/255 - 96s - loss: 1.1871 - accuracy: 0.7688 - val_loss: 3.0070 - val_accuracy: 0.3409\n",
            "Epoch 8/10\n",
            "255/255 - 96s - loss: 1.1063 - accuracy: 0.7925 - val_loss: 3.2186 - val_accuracy: 0.3170\n",
            "Epoch 9/10\n",
            "255/255 - 97s - loss: 1.0327 - accuracy: 0.8094 - val_loss: 3.1344 - val_accuracy: 0.3211\n",
            "Epoch 10/10\n",
            "255/255 - 97s - loss: 0.9589 - accuracy: 0.8286 - val_loss: 3.1077 - val_accuracy: 0.3262\n",
            "Training Group:  2\n",
            "Epoch 1/10\n",
            "255/255 - 97s - loss: 0.8910 - accuracy: 0.8525 - val_loss: 2.9509 - val_accuracy: 0.3429\n",
            "Epoch 2/10\n",
            "255/255 - 97s - loss: 0.8271 - accuracy: 0.8644 - val_loss: 3.0053 - val_accuracy: 0.3519\n",
            "Epoch 3/10\n",
            "255/255 - 97s - loss: 0.7790 - accuracy: 0.8759 - val_loss: 2.8895 - val_accuracy: 0.3656\n",
            "Epoch 4/10\n",
            "255/255 - 97s - loss: 0.7221 - accuracy: 0.8883 - val_loss: 2.8071 - val_accuracy: 0.3940\n",
            "Epoch 5/10\n",
            "255/255 - 97s - loss: 0.6711 - accuracy: 0.9026 - val_loss: 2.7575 - val_accuracy: 0.3861\n",
            "Epoch 6/10\n",
            "255/255 - 97s - loss: 0.6203 - accuracy: 0.9136 - val_loss: 2.8666 - val_accuracy: 0.3736\n",
            "Epoch 7/10\n",
            "255/255 - 97s - loss: 0.5769 - accuracy: 0.9258 - val_loss: 2.8571 - val_accuracy: 0.3677\n",
            "Epoch 8/10\n",
            "255/255 - 97s - loss: 0.5237 - accuracy: 0.9346 - val_loss: 2.9306 - val_accuracy: 0.3646\n",
            "Epoch 9/10\n",
            "255/255 - 97s - loss: 0.4951 - accuracy: 0.9392 - val_loss: 2.7743 - val_accuracy: 0.3899\n",
            "Epoch 10/10\n",
            "255/255 - 97s - loss: 0.4575 - accuracy: 0.9517 - val_loss: 2.9763 - val_accuracy: 0.3638\n",
            "Training Group:  3\n",
            "Epoch 1/10\n",
            "255/255 - 97s - loss: 0.4135 - accuracy: 0.9597 - val_loss: 2.5916 - val_accuracy: 0.4278\n",
            "Epoch 2/10\n",
            "255/255 - 97s - loss: 0.3903 - accuracy: 0.9619 - val_loss: 3.3329 - val_accuracy: 0.3186\n",
            "Epoch 3/10\n",
            "255/255 - 97s - loss: 0.3606 - accuracy: 0.9683 - val_loss: 2.7471 - val_accuracy: 0.3980\n",
            "Epoch 4/10\n",
            "255/255 - 97s - loss: 0.3301 - accuracy: 0.9716 - val_loss: 2.9704 - val_accuracy: 0.3677\n",
            "Epoch 5/10\n",
            "255/255 - 97s - loss: 0.3197 - accuracy: 0.9764 - val_loss: 2.6129 - val_accuracy: 0.4254\n",
            "Epoch 6/10\n",
            "255/255 - 97s - loss: 0.2848 - accuracy: 0.9816 - val_loss: 2.9299 - val_accuracy: 0.3611\n",
            "Epoch 7/10\n",
            "255/255 - 97s - loss: 0.2655 - accuracy: 0.9821 - val_loss: 2.6363 - val_accuracy: 0.4162\n",
            "Epoch 8/10\n",
            "255/255 - 97s - loss: 0.2430 - accuracy: 0.9839 - val_loss: 3.2788 - val_accuracy: 0.3192\n",
            "Epoch 9/10\n",
            "255/255 - 97s - loss: 0.2318 - accuracy: 0.9866 - val_loss: 2.3920 - val_accuracy: 0.4621\n",
            "Epoch 10/10\n",
            "255/255 - 97s - loss: 0.2098 - accuracy: 0.9885 - val_loss: 2.4637 - val_accuracy: 0.4456\n",
            "Training Group:  4\n",
            "Epoch 1/10\n",
            "255/255 - 97s - loss: 0.1942 - accuracy: 0.9904 - val_loss: 2.4321 - val_accuracy: 0.4633\n",
            "Epoch 2/10\n",
            "255/255 - 97s - loss: 0.1759 - accuracy: 0.9908 - val_loss: 2.5821 - val_accuracy: 0.4233\n",
            "Epoch 3/10\n",
            "255/255 - 96s - loss: 0.1619 - accuracy: 0.9932 - val_loss: 2.6766 - val_accuracy: 0.4118\n",
            "Epoch 4/10\n",
            "255/255 - 96s - loss: 0.1551 - accuracy: 0.9940 - val_loss: 2.9593 - val_accuracy: 0.3853\n",
            "Epoch 5/10\n",
            "255/255 - 97s - loss: 0.1479 - accuracy: 0.9917 - val_loss: 2.5396 - val_accuracy: 0.4443\n",
            "Epoch 6/10\n",
            "255/255 - 97s - loss: 0.1330 - accuracy: 0.9944 - val_loss: 2.5317 - val_accuracy: 0.4468\n",
            "Epoch 7/10\n",
            "255/255 - 97s - loss: 0.1299 - accuracy: 0.9944 - val_loss: 2.6241 - val_accuracy: 0.4304\n",
            "Epoch 8/10\n",
            "255/255 - 99s - loss: 0.1144 - accuracy: 0.9942 - val_loss: 2.7214 - val_accuracy: 0.4123\n",
            "Epoch 9/10\n",
            "255/255 - 100s - loss: 0.1169 - accuracy: 0.9940 - val_loss: 2.5856 - val_accuracy: 0.4361\n",
            "Epoch 10/10\n",
            "255/255 - 99s - loss: 0.1009 - accuracy: 0.9962 - val_loss: 2.5745 - val_accuracy: 0.4436\n",
            "Training Group:  5\n",
            "Epoch 1/10\n",
            "255/255 - 98s - loss: 0.0981 - accuracy: 0.9962 - val_loss: 2.4566 - val_accuracy: 0.4618\n",
            "Epoch 2/10\n",
            "255/255 - 97s - loss: 0.0907 - accuracy: 0.9958 - val_loss: 2.4023 - val_accuracy: 0.4702\n",
            "Epoch 3/10\n",
            "255/255 - 97s - loss: 0.0894 - accuracy: 0.9953 - val_loss: 2.8419 - val_accuracy: 0.3929\n",
            "Epoch 4/10\n",
            "255/255 - 97s - loss: 0.0824 - accuracy: 0.9968 - val_loss: 2.5890 - val_accuracy: 0.4358\n",
            "Epoch 5/10\n",
            "255/255 - 97s - loss: 0.0759 - accuracy: 0.9966 - val_loss: 2.5332 - val_accuracy: 0.4467\n",
            "Epoch 6/10\n",
            "255/255 - 97s - loss: 0.0698 - accuracy: 0.9968 - val_loss: 2.5420 - val_accuracy: 0.4502\n",
            "Epoch 7/10\n",
            "255/255 - 97s - loss: 0.0665 - accuracy: 0.9967 - val_loss: 2.4147 - val_accuracy: 0.4759\n",
            "Epoch 8/10\n",
            "255/255 - 97s - loss: 0.0682 - accuracy: 0.9959 - val_loss: 2.5279 - val_accuracy: 0.4557\n",
            "Epoch 9/10\n",
            "255/255 - 97s - loss: 0.0632 - accuracy: 0.9972 - val_loss: 2.3002 - val_accuracy: 0.4933\n",
            "Epoch 10/10\n",
            "255/255 - 97s - loss: 0.0695 - accuracy: 0.9962 - val_loss: 2.3317 - val_accuracy: 0.4875\n",
            "Training Group:  6\n",
            "Epoch 1/10\n",
            "255/255 - 97s - loss: 0.0562 - accuracy: 0.9971 - val_loss: 2.7154 - val_accuracy: 0.4201\n",
            "Epoch 2/10\n",
            "255/255 - 97s - loss: 0.0515 - accuracy: 0.9977 - val_loss: 2.6878 - val_accuracy: 0.4309\n",
            "Epoch 3/10\n",
            "255/255 - 97s - loss: 0.0471 - accuracy: 0.9973 - val_loss: 2.9229 - val_accuracy: 0.3968\n",
            "Epoch 4/10\n",
            "255/255 - 97s - loss: 0.0462 - accuracy: 0.9973 - val_loss: 2.3389 - val_accuracy: 0.4917\n",
            "Epoch 5/10\n",
            "255/255 - 97s - loss: 0.0441 - accuracy: 0.9975 - val_loss: 2.6882 - val_accuracy: 0.4298\n",
            "Epoch 6/10\n",
            "255/255 - 97s - loss: 0.0452 - accuracy: 0.9978 - val_loss: 3.3349 - val_accuracy: 0.3390\n",
            "Epoch 7/10\n",
            "255/255 - 97s - loss: 0.0471 - accuracy: 0.9968 - val_loss: 2.3589 - val_accuracy: 0.4935\n",
            "Epoch 8/10\n",
            "255/255 - 97s - loss: 0.0425 - accuracy: 0.9971 - val_loss: 3.3378 - val_accuracy: 0.3420\n",
            "Epoch 9/10\n",
            "255/255 - 97s - loss: 0.0399 - accuracy: 0.9974 - val_loss: 2.3307 - val_accuracy: 0.4963\n",
            "Epoch 10/10\n",
            "255/255 - 98s - loss: 0.0357 - accuracy: 0.9973 - val_loss: 2.5198 - val_accuracy: 0.4665\n",
            "Training Group:  7\n",
            "Epoch 1/10\n",
            "255/255 - 97s - loss: 0.0403 - accuracy: 0.9968 - val_loss: 2.8780 - val_accuracy: 0.4251\n",
            "Epoch 2/10\n",
            "255/255 - 97s - loss: 0.0327 - accuracy: 0.9978 - val_loss: 2.8491 - val_accuracy: 0.4196\n",
            "Epoch 3/10\n",
            "255/255 - 97s - loss: 0.0299 - accuracy: 0.9974 - val_loss: 2.9772 - val_accuracy: 0.3977\n",
            "Epoch 4/10\n",
            "255/255 - 97s - loss: 0.0338 - accuracy: 0.9972 - val_loss: 2.4436 - val_accuracy: 0.4802\n",
            "Epoch 5/10\n",
            "255/255 - 97s - loss: 0.0354 - accuracy: 0.9971 - val_loss: 3.0982 - val_accuracy: 0.3618\n",
            "Epoch 6/10\n",
            "255/255 - 97s - loss: 0.0352 - accuracy: 0.9968 - val_loss: 2.5823 - val_accuracy: 0.4652\n",
            "Epoch 7/10\n",
            "255/255 - 97s - loss: 0.0281 - accuracy: 0.9974 - val_loss: 2.4405 - val_accuracy: 0.4849\n",
            "Epoch 8/10\n",
            "255/255 - 97s - loss: 0.0239 - accuracy: 0.9980 - val_loss: 2.3700 - val_accuracy: 0.4861\n",
            "Epoch 9/10\n",
            "255/255 - 97s - loss: 0.0329 - accuracy: 0.9956 - val_loss: 3.4404 - val_accuracy: 0.3436\n",
            "Epoch 10/10\n",
            "255/255 - 97s - loss: 0.0285 - accuracy: 0.9977 - val_loss: 2.6452 - val_accuracy: 0.4512\n",
            "Training Group:  8\n",
            "Epoch 1/10\n",
            "255/255 - 97s - loss: 0.0280 - accuracy: 0.9968 - val_loss: 2.6086 - val_accuracy: 0.4519\n",
            "Epoch 2/10\n",
            "255/255 - 97s - loss: 0.0220 - accuracy: 0.9983 - val_loss: 2.9166 - val_accuracy: 0.4039\n",
            "Epoch 3/10\n",
            "255/255 - 97s - loss: 0.0229 - accuracy: 0.9978 - val_loss: 2.4493 - val_accuracy: 0.4875\n",
            "Epoch 4/10\n",
            "255/255 - 97s - loss: 0.0274 - accuracy: 0.9971 - val_loss: 2.7207 - val_accuracy: 0.4417\n",
            "Epoch 5/10\n",
            "255/255 - 97s - loss: 0.0248 - accuracy: 0.9980 - val_loss: 3.1826 - val_accuracy: 0.3845\n",
            "Epoch 6/10\n",
            "255/255 - 97s - loss: 0.0224 - accuracy: 0.9975 - val_loss: 2.3292 - val_accuracy: 0.5060\n",
            "Epoch 7/10\n",
            "255/255 - 97s - loss: 0.0222 - accuracy: 0.9975 - val_loss: 2.2869 - val_accuracy: 0.5100\n",
            "Epoch 8/10\n",
            "255/255 - 97s - loss: 0.0251 - accuracy: 0.9969 - val_loss: 6.0555 - val_accuracy: 0.1528\n",
            "Epoch 9/10\n",
            "255/255 - 97s - loss: 0.0267 - accuracy: 0.9969 - val_loss: 2.5424 - val_accuracy: 0.4799\n",
            "Epoch 10/10\n",
            "255/255 - 96s - loss: 0.0200 - accuracy: 0.9975 - val_loss: 2.9356 - val_accuracy: 0.4143\n",
            "Training Group:  9\n",
            "Epoch 1/10\n",
            "255/255 - 97s - loss: 0.0189 - accuracy: 0.9979 - val_loss: 2.6345 - val_accuracy: 0.4644\n",
            "Epoch 2/10\n",
            "255/255 - 97s - loss: 0.0203 - accuracy: 0.9977 - val_loss: 3.2189 - val_accuracy: 0.3676\n",
            "Epoch 3/10\n",
            "255/255 - 97s - loss: 0.0200 - accuracy: 0.9980 - val_loss: 2.5863 - val_accuracy: 0.4649\n",
            "Epoch 4/10\n",
            "255/255 - 97s - loss: 0.0231 - accuracy: 0.9979 - val_loss: 2.3544 - val_accuracy: 0.5086\n",
            "Epoch 5/10\n",
            "255/255 - 97s - loss: 0.0183 - accuracy: 0.9982 - val_loss: 2.4885 - val_accuracy: 0.4868\n",
            "Epoch 6/10\n",
            "255/255 - 97s - loss: 0.0184 - accuracy: 0.9977 - val_loss: 2.7065 - val_accuracy: 0.4445\n",
            "Epoch 7/10\n",
            "255/255 - 97s - loss: 0.0192 - accuracy: 0.9975 - val_loss: 2.8624 - val_accuracy: 0.4350\n",
            "Epoch 8/10\n",
            "255/255 - 97s - loss: 0.0181 - accuracy: 0.9974 - val_loss: 2.9483 - val_accuracy: 0.4118\n",
            "Epoch 9/10\n",
            "255/255 - 97s - loss: 0.0162 - accuracy: 0.9983 - val_loss: 2.4140 - val_accuracy: 0.5017\n",
            "Epoch 10/10\n",
            "255/255 - 97s - loss: 0.0158 - accuracy: 0.9980 - val_loss: 2.3819 - val_accuracy: 0.4984\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}